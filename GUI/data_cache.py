# C:\ë§¤ë§¤ì „ëµ\gui\data_manager.py

import sqlite3
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any, cast
from dataclasses import dataclass
import json
import time

# Logging
import logging
logger = logging.getLogger(__name__)

@dataclass
class CacheInfo:
    symbol: str
    timeframe: str
    exchange: str
    start_date: datetime
    end_date: datetime
    candle_count: int
    file_size: int  # bytes

class DataManager:
    """ë°ì´í„° ë‹¤ìš´ë¡œë“œ, ìºì‹œ, ë¡œë“œ í†µí•© ê´€ë¦¬"""
    
    TIMEFRAMES = ['1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d', '3d', '1w']
    
    def __init__(self, cache_dir: Optional[str] = None):
        if cache_dir:
            self.cache_dir = Path(cache_dir)
        else:
            # [FIX] EXE í™˜ê²½ ëŒ€ì‘
            import sys
            if getattr(sys, 'frozen', False):
                # PyInstaller EXEë¡œ ì‹¤í–‰ ì¤‘ â†’ EXE í´ë” ê¸°ì¤€
                base_dir = Path(sys.executable).parent
            else:
                # ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰ ì¤‘ â†’ ë§¤ë§¤ì „ëµ í´ë”
                base_dir = Path(__file__).resolve().parent.parent
            
            self.cache_dir = base_dir / "data" / "cache"
        
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logger.info(f"Warning: Could not create cache dir: {e}")
            # ëŒ€ì•ˆ: ì„ì‹œ í´ë” ì‚¬ìš©
            import tempfile
            self.cache_dir = Path(tempfile.gettempdir()) / "trading_cache"
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # [DEBUG] ê²½ë¡œ ì¶œë ¥
        logger.info(f"[DataManager] Cache Dir: {self.cache_dir}")
        
        self.exchange_manager = None
    
    def set_exchange(self, exchange_manager):
        """ExchangeManager ì—°ê²°"""
        self.exchange_manager = exchange_manager
    
    @property
    def _index_path(self) -> Path:
        """ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ"""
        return self.cache_dir / "cache_index.json"

    def _load_index(self) -> Dict:
        """ì¸ë±ìŠ¤ ë¡œë“œ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬)"""
        try:
            if self._index_path.exists():
                with open(self._index_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.info(f"âš ï¸ Index load failed: {e}")
        return {}

    def _save_index(self, index_data: Dict):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            with open(self._index_path, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, indent=2)
        except Exception as e:
            logger.info(f"âš ï¸ Index save failed: {e}")
    
    # íƒ€ì„í”„ë ˆì„ â†’ pandas ë¦¬ìƒ˜í”Œ ê·œì¹™
    TF_TO_PANDAS = {
        '1m': '1T', '3m': '3T', '5m': '5T', '15m': '15T', '30m': '30T',
        '1h': '1h', '2h': '2h', '4h': '4h', '6h': '6h', '12h': '12h',
        '1d': '1D', '3d': '3D', '1w': '1W'
    }
    
    def resample(self, df: pd.DataFrame, target_tf: str) -> pd.DataFrame:
        """15ë¶„ ë°ì´í„°ë¥¼ ìƒìœ„ íƒ€ì„í”„ë ˆì„ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
        
        Args:
            df: OHLCV ë°ì´í„°í”„ë ˆì„ (15m ê¸°ì¤€)
            target_tf: '1h', '4h', '1d' ë“±
            
        Returns:
            ë¦¬ìƒ˜í”Œëœ OHLCV ë°ì´í„°í”„ë ˆì„
        """
        if target_tf not in self.TF_TO_PANDAS:
            logger.info(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {target_tf}")
            return df
        
        rule = self.TF_TO_PANDAS[target_tf]
        
        # timestampë¥¼ datetime indexë¡œ ë³€í™˜
        df = df.copy()
        cast(Any, df)['datetime'] = pd.to_datetime(cast(Any, df)['timestamp'], unit='ms', utc=True)
        df = cast(Any, df).set_index('datetime')
        
        # OHLCV ë¦¬ìƒ˜í”Œë§
        resampled = df.resample(rule).agg({
            'timestamp': 'first',
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna()
        
        return resampled.reset_index(drop=True)
    
    def get_data(self, symbol: str, timeframe: str, exchange: str = 'bybit',
                 limit: int = 500, use_resample: bool = True) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ (15ë¶„ ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ ì§€ì›)
        
        Args:
            symbol: ì‹¬ë³¼
            timeframe: ì›í•˜ëŠ” íƒ€ì„í”„ë ˆì„
            exchange: ê±°ë˜ì†Œ
            limit: ìº”ë“¤ ìˆ˜
            use_resample: Trueë©´ 15ë¶„ ë°ì´í„°ì—ì„œ ë¦¬ìƒ˜í”Œë§
        """
        # 15ë¶„ ì´í•˜ë©´ ì§ì ‘ ë¡œë“œ
        if timeframe in ['1m', '3m', '5m', '15m'] or not use_resample:
            df = self.load(symbol, timeframe, exchange)
            if df.empty:
                 return df
            return df.tail(limit).reset_index(drop=True)
        
        # ìƒìœ„ TFëŠ” 15ë¶„ ë°ì´í„°ì—ì„œ ë¦¬ìƒ˜í”Œë§
        # í•„ìš”í•œ 15ë¶„ ìº”ë“¤ ìˆ˜ ê³„ì‚°
        tf_multiplier = {
            '30m': 2, '1h': 4, '2h': 8, '4h': 16,
            '6h': 24, '12h': 48, '1d': 96, '3d': 288, '1w': 672
        }
        
        multiplier = tf_multiplier.get(timeframe, 4)
        needed_15m = limit * multiplier
        
        # 15ë¶„ ë°ì´í„° ë¡œë“œ
        # 15ë¶„ ë°ì´í„° ë¡œë“œ (needed_15mì€ limitì´ ì•„ë‹˜, loadëŠ” ë‚ ì§œí•„í„°ë§Œ ìˆìŒ. 
        # ì—¬ê¸°ì„œëŠ” ì „ì²´ ë¡œë“œ í›„ tailí•˜ê±°ë‚˜, start_date ê³„ì‚°ì´ í•„ìš”í•¨. 
        # ì¼ë‹¨ ì „ì²´ ë¡œë“œ í›„ Slicingìœ¼ë¡œ ì²˜ë¦¬)
        df_15m = self.load(symbol, '15m', exchange)
        if not df_15m.empty:
            df_15m = df_15m.tail(needed_15m)
        
        if df_15m is None or len(df_15m) == 0:
            logger.info(f"âš ï¸ 15ë¶„ ë°ì´í„° ì—†ìŒ, ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
            df = self.load(symbol, timeframe, exchange)
            if df.empty:
                return df
            return df.tail(limit).reset_index(drop=True)
        
        # ë¦¬ìƒ˜í”Œë§
        resampled = self.resample(df_15m, timeframe)
        
        # ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        if len(resampled) > limit:
            resampled = resampled.tail(limit).reset_index(drop=True)
        
        logger.info(f"ğŸ“Š {symbol} {timeframe}: 15ë¶„â†’{len(resampled)}ê°œ ë¦¬ìƒ˜í”Œë§")
        return resampled
    
    def _get_cache_path(self, exchange: str, symbol: str, timeframe: str) -> Path:
        """ì •ê·œí™”ëœ ìºì‹œ íŒŒì¼ ê²½ë¡œ (Parquet í˜•ì‹)"""
        # ì‹¬ë³¼ ì •ê·œí™”: BTC/USDT:USDT â†’ btcusdt
        clean_symbol = symbol.replace('/', '').replace(':', '').lower()
        filename = f"{exchange.lower()}_{clean_symbol}_{timeframe}.parquet"
        return self.cache_dir / filename
    
    def download(self, symbol: str, timeframe: str, 
                 start_date: Optional[str] = None, end_date: Optional[str] = None,
                 exchange: str = "bybit", limit: Optional[int] = None,  # None = ì „ì²´ ìˆ˜ì§‘ (ìƒì¥ì¼ë¶€í„°)
                 progress_callback=None, processor=None) -> pd.DataFrame:
        """
        ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ì €ì¥
        
        Args:
            symbol: 'BTCUSDT' or 'BTC/USDT:USDT'
            timeframe: '15m', '1h', '4h', '1d' ë“±
            start_date: '2024-01-01' (ì—†ìœ¼ë©´ limit ê°œìˆ˜ë§Œí¼)
            end_date: '2024-12-31' (ì—†ìœ¼ë©´ í˜„ì¬)
            exchange: ê±°ë˜ì†Œëª…
            limit: ìµœëŒ€ ìº”ë“¤ ìˆ˜
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
            processor: ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ í•¨ìˆ˜ (ì˜ˆ: ì§€í‘œ ì¶”ê°€)
        """
        cache_path = self._get_cache_path(exchange, symbol, timeframe)
        
        # 1. ê¸°ì¡´ ìºì‹œ í™•ì¸
        existing_df = self._load_cache(cache_path)
        
        # [FIX] ìºì‹œê°€ ìˆë”ë¼ë„ ì‚¬ìš©ìê°€ ìš”ì²­í•œ start_dateê°€ ë” ê³¼ê±°ë¼ë©´ íˆìŠ¤í† ë¦¬ ì±„ìš°ê¸° ìˆ˜í–‰
        cache_start_ts = cast(Any, existing_df)['timestamp'].min() if existing_df is not None and not existing_df.empty else None
        
        if start_date:
            try:
                req_start_ts = int(pd.Timestamp(start_date).timestamp() * 1000)
            except Exception:

                req_start_ts = None
        else:
            req_start_ts = None

        if existing_df is not None and not existing_df.empty:
            # ìºì‹œê°€ ìˆê³ , ìš”ì²­í•œ ì‹œì‘ì¼ì´ ìºì‹œ ì‹œì‘ì ë³´ë‹¤ ì´í›„ë©´: ìµœì‹  ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸
            if req_start_ts is None or (cache_start_ts is not None and req_start_ts >= cache_start_ts):
                last_time = cast(Any, existing_df)['timestamp'].max()
                start_ts = int(last_time) + 1
                logger.info(f"ğŸ“¦ ìºì‹œ ì—…ë°ì´íŠ¸: {len(existing_df)}ê°œ ì´í›„ë¶€í„° ë‹¤ìš´ë¡œë“œ")
            else:
                # ìš”ì²­í•œ ì‹œì‘ì¼ì´ ìºì‹œë³´ë‹¤ ë” ê³¼ê±°ë¼ë©´: ê³¼ê±°ë¶€í„° ì „ì²´ ìˆ˜ì§‘ (ë³‘í•© ë¡œì§ì´ ì¤‘ë³µ ì œê±°í•¨)
                start_ts = req_start_ts
                cache_start_dt = datetime.fromtimestamp(cache_start_ts/1000) if cache_start_ts else 'unknown'
                logger.info(f"ğŸ“¦ íˆìŠ¤í† ë¦¬ ì±„ìš°ê¸°: ìºì‹œ ì‹œì‘ì ({cache_start_dt})ë³´ë‹¤ ê³¼ê±°ì¸ {start_date}ë¶€í„° ìˆ˜ì§‘ ì‹œì‘")
        else:
            start_ts = req_start_ts
            existing_df = pd.DataFrame()
        
        # 3. ìƒˆ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        new_data = self._fetch_ohlcv(
            symbol, timeframe, exchange, 
            since=start_ts, limit=limit,
            progress_callback=progress_callback
        )
        
        if new_data is None or len(new_data) == 0:
            logger.info("ğŸ“­ ìƒˆ ë°ì´í„° ì—†ìŒ")
            return existing_df
        
        new_df = pd.DataFrame(new_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        
        # 4. ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        if len(existing_df) > 0:
            combined = pd.concat([cast(Any, existing_df), cast(Any, new_df)], ignore_index=True)
            combined = cast(Any, combined).drop_duplicates(subset=['timestamp'], keep='last')
            combined = cast(Any, combined).sort_values('timestamp').reset_index(drop=True)
        else:
            combined = new_df
        
        # 4.5. ë°ì´í„° ê°€ê³µ (ì§€í‘œ ì¶”ê°€ ë“±)
        if processor:
            try:
                logger.info("âš™ï¸ ë°ì´í„° ê°€ê³µ ì¤‘...")
                combined = processor(combined)
            except Exception as e:
                logger.info(f"âš ï¸ ë°ì´í„° ê°€ê³µ ì‹¤íŒ¨: {e}")

        # 5. ìºì‹œ ì €ì¥
        self._save_cache(cache_path, combined)
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {len(combined)}ê°œ ìº”ë“¤ â†’ {cache_path.name}")
        
        # [NEW] ë¹—ì¸-ì—…ë¹„íŠ¸ í•˜ì´í¬ë¦¬ë“œ: ì—…ë¹„íŠ¸ ë°ì´í„°ë¥¼ ë¹—ì¸ íŒŒì¼ë¡œ ë™ì‹œ ë³µì‚¬
        # ì—…ë¹„íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹œ ë¹—ì¸ íŒŒì¼ë„ ê°±ì‹ , ë¹—ì¸ ë‹¤ìš´ë¡œë“œ(ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨) ì‹œì—ë„ ë¹—ì¸ íŒŒì¼ ì €ì¥
        try:
            try:
                from GUI.constants import COMMON_KRW_SYMBOLS
            except ImportError:
                from constants import COMMON_KRW_SYMBOLS
            
            coin = symbol.split('/')[0].replace('KRW', '').replace('-', '').upper()
            if coin in COMMON_KRW_SYMBOLS:
                if exchange.lower() == 'upbit':
                    bithumb_cache = self._get_cache_path('bithumb', coin, timeframe)
                    self._save_cache(bithumb_cache, combined)
                    logger.info(f"ğŸ”„ [HYBRID] Upbit data copied to Bithumb cache: {bithumb_cache.name}")
                elif exchange.lower() == 'bithumb':
                    upbit_cache = self._get_cache_path('upbit', coin, timeframe)
                    self._save_cache(upbit_cache, combined)
                    logger.info(f"ğŸ”„ [HYBRID] Bithumb(Redirected) data copied to Upbit cache: {upbit_cache.name}")
        except Exception as e:
            logger.info(f"âš ï¸ [HYBRID] Dual-saving failed: {e}")
            
        return combined
    
    # ì£¼ìš” ì½”ì¸ ìƒì¥ì¼ (í´ë°±ìš© - SymbolCache ì—†ì„ ë•Œ)
    COIN_LISTING_DATES = {
        'BTCUSDT': '2018-11-01',
        'ETHUSDT': '2018-11-01',
        'XRPUSDT': '2019-12-01',
        'SOLUSDT': '2021-06-01',
        'DOGEUSDT': '2021-06-02',
        'ADAUSDT': '2021-03-01',
        'AVAXUSDT': '2021-09-01',
        'DOTUSDT': '2021-01-01',
        'MATICUSDT': '2021-05-01',
        'LINKUSDT': '2020-08-01',
        'LTCUSDT': '2019-06-01',
        'ATOMUSDT': '2021-03-01',
        'UNIUSDT': '2021-02-01',
        'ETCUSDT': '2021-07-01',
        'APTUSDT': '2022-10-01',
        'ARBUSDT': '2023-03-01',
        'OPUSDT': '2022-06-01',
        'SUIUSDT': '2023-05-01',
        'NEARUSDT': '2021-10-01',
        'FILUSDT': '2021-04-01',
        'BNBUSDT': '2020-02-01',
    }
    
    def _get_listing_date(self, symbol: str, exchange: str = 'bybit') -> Optional[str]:
        """ì½”ì¸ ìƒì¥ì¼ ë°˜í™˜ (SymbolCache ìš°ì„ , í´ë°±ìœ¼ë¡œ í•˜ë“œì½”ë”©)"""
        clean = symbol.replace('/', '').replace(':', '').upper()
        
        # 1. SymbolCacheì—ì„œ ì¡°íšŒ
        try:
            from symbol_cache import get_symbol_cache
            cache = get_symbol_cache()
            
            # ccxt í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¡°íšŒ
            ccxt_symbol = f"{clean[:-4]}/{clean[-4:]}:{clean[-4:]}"
            listing = cache.get_listing_date(exchange, ccxt_symbol)
            if listing:
                return listing
        except Exception:
            pass  # Error silenced
        
        # 2. í•˜ë“œì½”ë”©ëœ ê°’ì—ì„œ ì¡°íšŒ
        if clean in self.COIN_LISTING_DATES:
            return self.COIN_LISTING_DATES[clean]
        
        # 3. ìƒì¥ì¼ ëª¨ë¥´ë©´ None ë°˜í™˜ -> _fetch_ohlcvì—ì„œ 2017ë…„ìœ¼ë¡œ ì²˜ë¦¬
        return None
    
    # ê±°ë˜ì†Œë³„ ìº”ë“¤ ìš”ì²­ ì œí•œ (ì•ˆì „í•œ ê°’)
    EXCHANGE_LIMITS = {
        'bithumb': 200,   # ë¹—ì¸ì€ 200ê°œê°€ ê°€ì¥ ì•ˆì •ì ì„
        'upbit': 1000,    # ì—…ë¹„íŠ¸ëŠ” ìµœëŒ€ 1000ê°œê¹Œì§€ ì§€ì› (ì¼ë¶€ ìº”ë“¤)
        'binance': 1000,
        'bybit': 1000,
        'okx': 100,       # OKXëŠ” í˜ì´ì§€ë‹¹ ì œí•œì´ ì—„ê²©í•¨
        'bitget': 1000,
        'bingx': 1000,
    }

    def _fetch_upbit_pyupbit(self, symbol: str, timeframe: str, since: Optional[int] = None, limit: Optional[int] = 1000, progress_callback=None) -> List:
        """Pyupbitë¥¼ ì‚¬ìš©í•œ Upbit ë°ì´í„° ìˆ˜ì§‘ (Pagination ì§€ì›)"""
        try:
            import pyupbit
            import time
            from datetime import datetime, timedelta
            
            # 1. Interval ë³€í™˜
            # pyupbit: minute1, minute3, ..., day, week, month
            tf_map = {
                '1m': 'minute1', '3m': 'minute3', '5m': 'minute5', 
                '15m': 'minute15', '30m': 'minute30', '1h': 'minute60',
                '4h': 'minute240', '1d': 'day', '1w': 'week'
            }
            interval = tf_map.get(timeframe, 'minute60')
            
            # 2. Symbol ë³€í™˜ (BTC/KRW -> KRW-BTC)
            if '/' in symbol:
                coin, base = symbol.split('/')
                symbol = f"{base}-{coin}"
            
            all_df = pd.DataFrame()
            to_date = datetime.now()
            
            fetched_count = 0
            
            logger.info(f"ğŸ“¥ [Upbit-Pyupbit] {symbol} {interval} ìˆ˜ì§‘ ì‹œì‘ (Target: {limit})")
            
            # 3. Backward Loop
            safe_limit = limit if limit is not None else 1000  # Default fallback if None
            while fetched_count < safe_limit:
                # í•œ ë²ˆì— 200ê°œ (Upbit API ì œí•œ)
                count = min(200, safe_limit - fetched_count)
                
                df = pyupbit.get_ohlcv(symbol, interval=interval, to=to_date, count=count)
                
                if df is None or df.empty:
                    break
                    
                # index(datetime)ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ
                df = df.reset_index().rename(columns={'index': 'timestamp'})
                
                # timestampë¥¼ ms intë¡œ ë³€í™˜
                df['timestamp'] = cast(Any, df['timestamp']).astype(np.int64) // 10**6
                
                # since í•„í„°ë§
                if since:
                    df = df[df['timestamp'] >= since]
                    if df.empty:
                        # ìš”ì²­í•œ sinceë³´ë‹¤ ë‹¤ ì´ì „ ë°ì´í„°ë§Œ ë‚¨ìœ¼ë©´ ì¢…ë£Œ
                        break
                
                # ë³‘í•©
                all_df = pd.concat([df, all_df]) # ë’¤ì—ì„œë¶€í„° ê°€ì ¸ì˜¤ë¯€ë¡œ ì•ì— ë¶™ì„? ì•„ë‹ˆ, ì¤‘ë³µ ì œê±° í›„ ì •ë ¬ì´ ë‚˜ìŒ. 
                # ì¼ë‹¨ ê·¸ëƒ¥ ëª¨ìœ¼ê³  ë‚˜ì¤‘ì— ì •ë¦¬. í•˜ì§€ë§Œ to_date ê°±ì‹ ì„ ìœ„í•´ ê°€ì¥ ê³¼ê±° ë°ì´í„° í•„ìš”.
                
                fetched_count += len(df)
                if progress_callback:
                    progress_callback(fetched_count)
                
                # ì»¤ì„œ ê°±ì‹  (ê°€ì¥ ê³¼ê±° ì‹œê°„)
                first_ts = df.iloc[0]['timestamp']
                to_date = datetime.fromtimestamp(first_ts / 1000) - timedelta(seconds=1) # 1ì´ˆ ì „
                
                # sinceë³´ë‹¤ ë” ê³¼ê±°ë¡œ ê°”ìœ¼ë©´ ì¢…ë£Œ
                if since and first_ts < since:
                    break
                
                time.sleep(0.1) # Rate Limit ì¤€ìˆ˜
            
            if all_df.empty:
                return []
                
            # ì •ë¦¬
            all_df = all_df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
            
            # ë¦¬ìŠ¤íŠ¸ ë³€í™˜ [[ts, o, h, l, c, v], ...]
            # pyupbit ì»¬ëŸ¼: open, high, low, close, volume, value(ê±°ë˜ëŒ€ê¸ˆ)
            result = all_df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].values.tolist()
            
            logger.info(f"âœ… [Upbit-Pyupbit] ìˆ˜ì§‘ ì™„ë£Œ: {len(result)}ê°œ")
            return result
            
        except ImportError:
            logger.info("âŒ pyupbit not installed")
            return []
        except Exception as e:
            logger.info(f"âŒ [Upbit-Pyupbit] Error: {e}")
            return []

    def _fetch_ohlcv(self, symbol: str, timeframe: str, exchange: str,
                     since: Optional[int] = None, limit: Optional[int] = 1000,
                     progress_callback=None) -> List:
        """OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ccxt ì‚¬ìš©)"""
        try:
            import ccxt
            
            exchange_id = exchange.lower()
            
            # [NEW] ë¹—ì¸-ì—…ë¹„íŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë‹¤ì´ë ‰ì…˜
            # ë¹—ì¸ì˜ ê²½ìš° ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë¯€ë¡œ, ì—…ë¹„íŠ¸ ê³µí†µ ì½”ì¸ì€ ì—…ë¹„íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
            if exchange_id == 'bithumb':
                try:
                    try:
                        from GUI.constants import COMMON_KRW_SYMBOLS
                    except ImportError:
                        from constants import COMMON_KRW_SYMBOLS
                    # ì‹¬ë³¼ì—ì„œ ì½”ì¸ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: BTC/KRW -> BTC, BTC -> BTC)
                    coin = symbol.split('/')[0].replace('KRW', '').replace('-', '').upper()
                    if coin in COMMON_KRW_SYMBOLS:
                        logger.info(f"ğŸ”„ [HYBRID] Bithumb {coin} -> Switching to Upbit Data Source")
                        exchange_id = 'upbit'
                        # ì—…ë¹„íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‹¬ë³¼ ë³€í™˜
                        symbol = f"{coin}/KRW"
                except Exception as e:
                    logger.info(f"âš ï¸ [HYBRID] Redirection failed: {e}")

            # [NEW] Upbit ì „ìš© ì²˜ë¦¬ (Pyupbit + Pagination)
            if exchange_id == 'upbit':
                return self._fetch_upbit_pyupbit(symbol, timeframe, since, limit, progress_callback)
            
            # [FIX] KRW ê±°ë˜ì†Œ ì²´í¬
            is_krw_exchange = exchange_id in ['bithumb', 'upbit']
            
            # ê±°ë˜ì†Œ ì¸ìŠ¤í„´ìŠ¤
            exchange_class = getattr(ccxt, exchange_id)
            ex = exchange_class({
                'enableRateLimit': True,
                'options': {'defaultType': 'spot' if is_krw_exchange else 'swap', 'adjustForTimeDifference': True},
                'timeout': 30000
            })
            ex.load_markets()
            
            # ì‹¬ë³¼ ë³€í™˜
            original_symbol = symbol
            if '/' not in symbol:
                if is_krw_exchange:
                    # [FIX] KRW-BTC â†’ BTC/KRW (ë¹—ì¸/ì—…ë¹„íŠ¸)
                    if symbol.startswith('KRW-'):
                        base = symbol.replace('KRW-', '')
                        symbol = f"{base}/KRW"
                    else:
                        # BTCKRW â†’ BTC/KRW
                        if symbol.endswith('KRW'):
                            symbol = f"{symbol[:-3]}/KRW"
                        else:
                            # ê¸°ë³¸ì ìœ¼ë¡œ BTC/KRW í˜•ì‹ìœ¼ë¡œ ì‹œë„
                            symbol = f"{symbol}/KRW"
                else:
                    # BTCUSDT â†’ BTC/USDT:USDT
                    symbol = f"{symbol[:-4]}/{symbol[-4:]}:{symbol[-4:]}"
            
            # ìƒì¥ì¼ ì²´í¬
            listing_date = self._get_listing_date(original_symbol, exchange_id)
            listing_ts = None
            if listing_date:
                try:
                    # [FIX] Handle NaT safely (convert to str to avoid ambiguous boolean eval)
                    ts = pd.Timestamp(listing_date)
                    if str(ts) != 'NaT':
                        listing_ts = int(ts.timestamp() * 1000)
                except Exception:
                    pass  # Error silenced
            
            # since ì„¤ì •
            if since is None:
                if listing_ts:
                    since = listing_ts
                else:
                    # [FIX] ìƒì¥ì¼ ë¶ˆëª…í™• ì‹œ 2017.01.01ë¶€í„° ì „ì²´ ìˆ˜ì§‘ (ê¸°ì¡´ 2ë…„ ì œí•œ í•´ì œ)
                    since = int(pd.Timestamp("2017-01-01").timestamp() * 1000)
            elif listing_ts and since < listing_ts:
                since = listing_ts
            
            all_data = []
            fetched = 0
            
            # [FIX] limitì´ Noneì¼ ê²½ìš° ë¬´í•œ ìˆ˜ì§‘ì„ ìœ„í•´ ë§¤ìš° í° ê°’ìœ¼ë¡œ ì„¤ì •
            if limit is None:
                limit = 1000000 
            
            # [FIX] ê±°ë˜ì†Œë³„ ë§ì¶¤í˜• ë°°ì¹˜ ì‚¬ì´ì¦ˆ
            batch_size = self.EXCHANGE_LIMITS.get(exchange_id, 1000)
            # ìš”ì²­í•œ limitë³´ë‹¤ batch_sizeê°€ í¬ë©´ ì¡°ì ˆ
            batch_size = min(batch_size, limit)
            
            logger.info(f"ğŸ“¥ {exchange_id} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {symbol} ({timeframe}), Target: {limit}, Batch: {batch_size}")
            
            retry_count = 0
            max_retries = 3
            
            while fetched < limit:
                try:
                    # ë‚¨ì€ ê°œìˆ˜ê°€ batch_sizeë³´ë‹¤ ì‘ìœ¼ë©´ ì¡°ì ˆ
                    current_batch = min(batch_size, limit - fetched)
                    
                    # [DEBUG] Readable timestamp
                    since_ts = since if since else 0
                    readable_since = datetime.fromtimestamp(since_ts/1000).strftime('%Y-%m-%d %H:%M')
                    logger.info(f"[DataManager] Batch {fetched//batch_size + 1}: {fetched:,}/{limit:,} candles (since={readable_since})")
                    data = ex.fetch_ohlcv(symbol, timeframe, since=since, limit=current_batch)
                    retry_count = 0
                except Exception as e:
                    retry_count += 1
                    if retry_count >= max_retries:
                        logger.info(f"âŒ {exchange_id} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                        break
                    logger.info(f"âš ï¸ ì¬ì‹œë„ {retry_count}/{max_retries}...")
                    time.sleep(2)
                    continue
                
                if not data:
                    # [FIX] ë°ì´í„°ê°€ ì—†ëŠ”ë° ì²˜ìŒ(fetched=0)ì¸ ê²½ìš°ì—ë§Œ 1íšŒì„± í´ë°± ì‹œë„
                    if fetched == 0 and since is not None:
                        # [CAUTION] 'since=None'ì€ ê±°ë˜ì†Œì— ë”°ë¼ ìµœì‹  1000ê°œë§Œ ë°˜í™˜í•˜ê³  ëë‚  ìˆ˜ ìˆìŒ
                        # ì—¬ê¸°ì„œëŠ” logë§Œ ë‚¨ê¸°ê³  ë£¨í”„ë¥¼ ì¢…ë£Œí•˜ê±°ë‚˜, ë‹¤ë¥¸ ì „ëµì„ ì·¨í•´ì•¼ í•¨
                        since_ts = since if since else 0
                        logger.info(f"â„¹ï¸ {exchange_id}: {datetime.fromtimestamp(since_ts/1000)} ì´í›„ ë°ì´í„° ì—†ìŒ (ìƒì¥ ì „ ë˜ëŠ” ì„œë²„ ì œí•œ)")
                    break
                
                # [FIX] CCXT í˜ì´ì§€ë„¤ì´ì…˜ì˜ ê³ ì§ˆì ì¸ ì¤‘ë³µ ì²´í¬ ê°œì„ 
                # ì¤‘ë³µ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ë¬´í•œ ë£¨í”„ ë°©ì§€ & ì‹¤ì œ ë°ì´í„°ë§Œ ì¶”ê°€
                new_candles = []
                for d in data:
                    if not all_data or d[0] > all_data[-1][0]:
                        new_candles.append(d)
                
                if not new_candles:
                    logger.info("â„¹ï¸ ë” ì´ìƒ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìˆ˜ì§‘ ì¢…ë£Œ)")
                    break
                    
                all_data.extend(new_candles)
                fetched += len(new_candles)
                
                if progress_callback:
                    progress_callback(fetched)
                
                # ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•œ since ì—…ë°ì´íŠ¸
                last_ts = all_data[-1][0]
                
                # í˜„ì¬ ì‹œê°„ì— ë„ë‹¬í–ˆëŠ”ì§€ ì²´í¬ (10ì´ˆ ì „ê¹Œì§€ ìˆ˜ì§‘)
                now_ts = int(time.time() * 1000)
                if last_ts >= now_ts - 20000: # 20ì´ˆ ì´ë‚´
                    logger.info("âœ… í˜„ì¬ ì‹œì ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                    break
                    
                since = last_ts + 1
                time.sleep(0.1) # ì†ë„ ê°œì„  (ê¸°ì¡´ 0.3 -> 0.1)
            
            # ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ìƒí™© ëŒ€ë¹„)
            if all_data:
                # [FIX] Cast columns to Any to satisfy Pyright
                from typing import cast, Any
                cols = cast(Any, ['ts', 'o', 'h', 'l', 'c', 'v'])
                df_temp = pd.DataFrame(all_data, columns=cols)
                df_temp = df_temp.drop_duplicates(subset=['ts']).sort_values('ts')
                all_data = df_temp.values.tolist()
                
            logger.info(f"âœ… {exchange_id} ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_data)}ê°œ ìº”ë“¤")
            return all_data
            
        except Exception as e:
            logger.info(f"âŒ {symbol} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _load_cache(self, cache_path: Path) -> Optional[pd.DataFrame]:
        """Parquet ìºì‹œ ë¡œë“œ (ë ˆê±°ì‹œ SQLite í˜¸í™˜)"""
        # 1. Parquet íŒŒì¼ í™•ì¸
        if cache_path.exists():
            try:
                df = pd.read_parquet(cache_path)
                # [FIX] Timestamp í˜•ì‹ì´ datetimeì¸ ê²½ìš° int64(ms)ë¡œ ì •ê·œí™”
                if 'timestamp' in df.columns:
                    if pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                        # Explicit conversion to avoid "NaTType" access issues
                        df['timestamp'] = df['timestamp'].view(np.int64) // 10**6
                return df
            except Exception as e:
                logger.info(f"âš ï¸ Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 2. ë ˆê±°ì‹œ SQLite íŒŒì¼ í™•ì¸ (ë§ˆì´ê·¸ë ˆì´ì…˜ ì§€ì›)
        legacy_path = cache_path.with_suffix('.db')
        if legacy_path.exists():
            try:
                conn = sqlite3.connect(legacy_path)
                df = pd.read_sql("SELECT * FROM ohlcv ORDER BY timestamp", conn)
                conn.close()
                logger.info(f"ğŸ“¦ ë ˆê±°ì‹œ DB â†’ Parquet ë³€í™˜: {legacy_path.name}")
                # ìë™ ë³€í™˜
                self._save_cache(cache_path, df)
                return df
            except Exception:
                pass  # Error silenced
        
        return None
    
    def _save_cache(self, cache_path: Path, df: pd.DataFrame):
        """Parquet ìºì‹œ ì €ì¥ (ì••ì¶•)"""
        df.to_parquet(cache_path, index=False, compression='snappy')
        logger.info(f"ğŸ’¾ Parquet ì €ì¥: {cache_path.name} ({len(df):,}í–‰)")
    
    def load(self, symbol: str, timeframe: str, exchange: str = "bybit",
             start_date: Optional[str] = None, end_date: Optional[str] = None) -> pd.DataFrame:
        """ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ DataFrame)"""
        cache_path = self._get_cache_path(exchange, symbol, timeframe)
        df = self._load_cache(cache_path)
        
        if df is None:
            return pd.DataFrame()
        
        # ë‚ ì§œ í•„í„°ë§
        if start_date:
            start_ts = pd.Timestamp(start_date).timestamp() * 1000
            df = df[df['timestamp'] >= start_ts]
        if end_date:
            end_ts = pd.Timestamp(end_date).timestamp() * 1000
            df = df[df['timestamp'] <= end_ts]
        
        return df
    
    def load_data(self, symbol: str, exchange_id: str, timeframe: str,
                  start_date: Optional[str] = None, end_date: Optional[str] = None) -> pd.DataFrame:
        """backtest_widget í˜¸í™˜ìš© load_data ë©”ì„œë“œ
        
        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'BTC/USDT:USDT' ë˜ëŠ” 'BTCUSDT')
            exchange_id: ê±°ë˜ì†Œ ID (ì˜ˆ: 'binance', 'bybit')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '15m', '1h')
            start_date: ì‹œì‘ì¼ (ì˜ˆ: '2024-01-01')
            end_date: ì¢…ë£Œì¼
        """
        # ì‹¬ë³¼ ì •ê·œí™” (BTC/USDT:USDT â†’ btcusdt, BTCUSDT â†’ btcusdt)
        normalized_symbol = symbol.replace('/', '').replace(':', '').lower()
        
        return self.load(
            symbol=normalized_symbol,
            timeframe=timeframe,
            exchange=exchange_id.lower(),
            start_date=start_date,
            end_date=end_date
        )
    

    def _get_db_metadata(self, db_path: Path):
        """DB ë©”íƒ€ë°ì´í„°ë§Œ ë¹ ë¥´ê²Œ ì¡°íšŒ (Parquet)"""
        try:
            # Parquet íŒŒì¼ì—ì„œ timestamp ì»¬ëŸ¼ë§Œ ì½ì–´ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            df = pd.read_parquet(db_path, columns=['timestamp'])
            if not df.empty:
                # [FIX] Timestamp í˜•ì‹ì´ datetimeì¸ ê²½ìš° int64(ms)ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°
                ts_col = df['timestamp']
                if pd.api.types.is_datetime64_any_dtype(ts_col):
                    ts_col = ts_col.astype(np.int64) // 10**6
                
                min_ts = int(ts_col.min())
                max_ts = int(ts_col.max())
                count = len(df)
                return min_ts, max_ts, count
            return None, None, 0
        except Exception as e:
            # ì½ê¸° ì‹¤íŒ¨ ì‹œ
            return None, None, 0

    def get_cache_list(self) -> List[CacheInfo]:
        """ìºì‹œëœ ë°ì´í„° ëª©ë¡ (ì¸ë±ì‹± ì‹œìŠ¤í…œ ì ìš© v1.6.0)"""
        cache_list = []
        index = self._load_index()
        dirty = False
        
        # í˜„ì¬ ì¡´ì¬í•˜ëŠ” Parquet íŒŒì¼ ìŠ¤ìº”
        current_files = set()
        
        for db_file in self.cache_dir.glob("*.parquet"):
            current_files.add(db_file.name)
            
            try:
                # íŒŒì¼ ìƒíƒœ í™•ì¸
                stat = db_file.stat()
                mtime = stat.st_mtime
                size = stat.st_size
                
                # ë„ˆë¬´ ì‘ì€ íŒŒì¼ ìŠ¤í‚µ
                if size < 1024:
                    continue
                
                filename = db_file.name
                
                # ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬ (íŒŒì¼ëª… ìˆê³ , ìˆ˜ì •ì‹œê°„/í¬ê¸° ì¼ì¹˜)
                use_index = False
                if filename in index:
                    cached = index[filename]
                    if cached.get('mtime') == mtime and cached.get('size') == size:
                        use_index = True
                
                if use_index:
                    # ì¸ë±ìŠ¤ ë°ì´í„° ì‚¬ìš©
                    cached = index[filename]
                    # datetime ë³µì›
                    s_date = datetime.fromtimestamp(cached['start_ts'])
                    e_date = datetime.fromtimestamp(cached['end_ts'])
                    
                    cache_list.append(CacheInfo(
                        symbol=cached['symbol'],
                        timeframe=cached['timeframe'],
                        exchange=cached['exchange'],
                        start_date=s_date,
                        end_date=e_date,
                        candle_count=cached['count'],
                        file_size=size
                    ))
                else:
                    # ë©”íƒ€ë°ì´í„° ì§ì ‘ ì½ê¸° (Costly I/O)
                    parts = db_file.stem.split('_')
                    if len(parts) >= 3:
                        exchange = parts[0]
                        symbol = parts[1].upper()
                        timeframe = parts[-1]
                        
                        min_ts, max_ts, count = self._get_db_metadata(db_file)
                        
                        if count > 0 and min_ts and max_ts:
                            # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                            index[filename] = {
                                'symbol': symbol,
                                'timeframe': timeframe,
                                'exchange': exchange,
                                'count': count,
                                'start_ts': min_ts / 1000, # ms -> sec (for datetime) -> Wait, CacheInfo takes datetime object.
                                                           # let's store timestamp (sec) in index to be safe/standard
                                'end_ts': max_ts / 1000,
                                'mtime': mtime,
                                'size': size
                            }
                            dirty = True
                            
                            cache_list.append(CacheInfo(
                                symbol=symbol,
                                timeframe=timeframe,
                                exchange=exchange,
                                start_date=datetime.utcfromtimestamp(min_ts / 1000),
                                end_date=datetime.utcfromtimestamp(max_ts / 1000),
                                candle_count=count,
                                file_size=size
                            ))
            except Exception as e:
                # logger.info(f"Error processing {db_file.name}: {e}")
                continue
                
        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        for filename in list(index.keys()):
            if filename not in current_files:
                del index[filename]
                dirty = True
        
        # ë³€ê²½ì‚¬í•­ ì €ì¥
        if dirty:
            self._save_index(index)
        
        return cache_list
    
    def delete_cache(self, exchange: str, symbol: str, timeframe: str) -> bool:
        """ìºì‹œ ì‚­ì œ"""
        cache_path = self._get_cache_path(exchange, symbol, timeframe)
        if cache_path.exists():
            cache_path.unlink()
            return True
        return False
    
    def cleanup_duplicates(self, dry_run: bool = True) -> Dict:
        """ì¤‘ë³µ ìºì‹œ ì •ë¦¬"""
        from collections import defaultdict
        
        # ì •ê·œí™”ëœ í‚¤ë¡œ ê·¸ë£¹í•‘
        groups = defaultdict(list)
        for db_file in self.cache_dir.glob("*.parquet"):
            parts = db_file.stem.lower().split('_')
            if len(parts) >= 3:
                key = f"{parts[0]}_{parts[1]}_{parts[-1]}"
                groups[key].append(db_file)
        
        # ì¤‘ë³µ ì°¾ê¸°
        duplicates = {k: v for k, v in groups.items() if len(v) > 1}
        
        result = {'found': len(duplicates), 'deleted': 0, 'kept': []}
        
        for key, files in duplicates.items():
            # ê°€ì¥ í° íŒŒì¼ ìœ ì§€
            files.sort(key=lambda f: f.stat().st_size, reverse=True)
            keep = files[0]
            delete = files[1:]
            
            result['kept'].append(keep.name)
            
            if not dry_run:
                for f in delete:
                    f.unlink()
                    result['deleted'] += 1
        
        return result
    
    def get_all_cache_list(self):
        """cache_manager_widgetê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        cache_list = self.get_cache_list()
        result = []
        
        for cache in cache_list:
            result.append({
                'exchange': cache.exchange,
                'symbol': cache.symbol,
                'timeframe': cache.timeframe,
                'first_date': cache.start_date.strftime('%Y-%m-%d'),
                'last_date': cache.end_date.strftime('%Y-%m-%d'),
                'count': cache.candle_count,
                'file_size': cache.file_size / (1024 * 1024),  # MB
                'filename': f"{cache.exchange}_{cache.symbol.lower()}_{cache.timeframe}.parquet"
            })
        
        return result
    
    # cache_manager_widgetê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±
    @property
    def CACHE_DIR(self):
        return self.cache_dir


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    dm = DataManager()
    
    logger.info(f"ğŸ“‚ ìºì‹œ ë””ë ‰í† ë¦¬: {dm.cache_dir}")
    logger.info(f"ğŸ“Š ì§€ì› íƒ€ì„í”„ë ˆì„: {dm.TIMEFRAMES}")
    
    # ìºì‹œ ëª©ë¡
    cache_list = dm.get_cache_list()
    logger.info(f"\nğŸ“¦ ìºì‹œëœ ë°ì´í„°: {len(cache_list)}ê°œ")
    for c in cache_list[:5]:
        logger.info(f"  - {c.exchange} {c.symbol} {c.timeframe}: {c.candle_count}ê°œ")
    
    # ì¤‘ë³µ ì²´í¬
    dup_result = dm.cleanup_duplicates(dry_run=True)
    logger.info(f"\nğŸ” ì¤‘ë³µ ìºì‹œ: {dup_result['found']}ê°œ ê·¸ë£¹")
    
    # ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ (API ì—†ì´)
    logger.info("\nğŸ“¥ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    df = dm.download('BTCUSDT', '15m', exchange='bybit', limit=100)
    if len(df) > 0:
        logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {len(df)}ê°œ ìº”ë“¤")
        logger.info(df.tail(3))
