# C:\ë§¤ë§¤ì „ëµ\gui\data_manager.py

import sqlite3
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict
from dataclasses import dataclass
import json
import time

@dataclass
class CacheInfo:
    symbol: str
    timeframe: str
    exchange: str
    start_date: datetime
    end_date: datetime
    candle_count: int
    file_size: int  # bytes

class DataManager:
    """ë°ì´í„° ë‹¤ìš´ë¡œë“œ, ìºì‹œ, ë¡œë“œ í†µí•© ê´€ë¦¬"""
    
    TIMEFRAMES = ['1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d', '3d', '1w']
    
    def __init__(self, cache_dir: str = None):
        if cache_dir:
            self.cache_dir = Path(cache_dir)
        else:
            # [FIX] EXE í™˜ê²½ ëŒ€ì‘
            import sys
            if getattr(sys, 'frozen', False):
                # PyInstaller EXEë¡œ ì‹¤í–‰ ì¤‘ â†’ EXE í´ë” ê¸°ì¤€
                base_dir = Path(sys.executable).parent
            else:
                # ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰ ì¤‘ â†’ ë§¤ë§¤ì „ëµ í´ë”
                base_dir = Path(__file__).resolve().parent.parent
            
            self.cache_dir = base_dir / "data" / "cache"
        
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            print(f"Warning: Could not create cache dir: {e}")
            # ëŒ€ì•ˆ: ì„ì‹œ í´ë” ì‚¬ìš©
            import tempfile
            self.cache_dir = Path(tempfile.gettempdir()) / "trading_cache"
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # [DEBUG] ê²½ë¡œ ì¶œë ¥
        print(f"[DataManager] Cache Dir: {self.cache_dir}")
        
        self.exchange_manager = None
    
    def set_exchange(self, exchange_manager):
        """ExchangeManager ì—°ê²°"""
        self.exchange_manager = exchange_manager
    
    # íƒ€ì„í”„ë ˆì„ â†’ pandas ë¦¬ìƒ˜í”Œ ê·œì¹™
    TF_TO_PANDAS = {
        '1m': '1T', '3m': '3T', '5m': '5T', '15m': '15T', '30m': '30T',
        '1h': '1h', '2h': '2h', '4h': '4h', '6h': '6h', '12h': '12h',
        '1d': '1D', '3d': '3D', '1w': '1W'
    }
    
    def resample(self, df: pd.DataFrame, target_tf: str) -> pd.DataFrame:
        """15ë¶„ ë°ì´í„°ë¥¼ ìƒìœ„ íƒ€ì„í”„ë ˆì„ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
        
        Args:
            df: OHLCV ë°ì´í„°í”„ë ˆì„ (15m ê¸°ì¤€)
            target_tf: '1h', '4h', '1d' ë“±
            
        Returns:
            ë¦¬ìƒ˜í”Œëœ OHLCV ë°ì´í„°í”„ë ˆì„
        """
        if target_tf not in self.TF_TO_PANDAS:
            print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {target_tf}")
            return df
        
        rule = self.TF_TO_PANDAS[target_tf]
        
        # timestampë¥¼ datetime indexë¡œ ë³€í™˜
        df = df.copy()
        df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
        df = df.set_index('datetime')
        
        # OHLCV ë¦¬ìƒ˜í”Œë§
        resampled = df.resample(rule).agg({
            'timestamp': 'first',
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna()
        
        return resampled.reset_index(drop=True)
    
    def get_data(self, symbol: str, timeframe: str, exchange: str = 'bybit',
                 limit: int = 500, use_resample: bool = True) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ (15ë¶„ ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ ì§€ì›)
        
        Args:
            symbol: ì‹¬ë³¼
            timeframe: ì›í•˜ëŠ” íƒ€ì„í”„ë ˆì„
            exchange: ê±°ë˜ì†Œ
            limit: ìº”ë“¤ ìˆ˜
            use_resample: Trueë©´ 15ë¶„ ë°ì´í„°ì—ì„œ ë¦¬ìƒ˜í”Œë§
        """
        # 15ë¶„ ì´í•˜ë©´ ì§ì ‘ ë¡œë“œ
        if timeframe in ['1m', '3m', '5m', '15m'] or not use_resample:
            return self.load(symbol, timeframe, exchange, limit)
        
        # ìƒìœ„ TFëŠ” 15ë¶„ ë°ì´í„°ì—ì„œ ë¦¬ìƒ˜í”Œë§
        # í•„ìš”í•œ 15ë¶„ ìº”ë“¤ ìˆ˜ ê³„ì‚°
        tf_multiplier = {
            '30m': 2, '1h': 4, '2h': 8, '4h': 16,
            '6h': 24, '12h': 48, '1d': 96, '3d': 288, '1w': 672
        }
        
        multiplier = tf_multiplier.get(timeframe, 4)
        needed_15m = limit * multiplier
        
        # 15ë¶„ ë°ì´í„° ë¡œë“œ
        df_15m = self.load(symbol, '15m', exchange, needed_15m)
        
        if df_15m is None or len(df_15m) == 0:
            print(f"âš ï¸ 15ë¶„ ë°ì´í„° ì—†ìŒ, ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
            return self.load(symbol, timeframe, exchange, limit)
        
        # ë¦¬ìƒ˜í”Œë§
        resampled = self.resample(df_15m, timeframe)
        
        # ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        if len(resampled) > limit:
            resampled = resampled.tail(limit).reset_index(drop=True)
        
        print(f"ğŸ“Š {symbol} {timeframe}: 15ë¶„â†’{len(resampled)}ê°œ ë¦¬ìƒ˜í”Œë§")
        return resampled
    
    def _get_cache_path(self, exchange: str, symbol: str, timeframe: str) -> Path:
        """ì •ê·œí™”ëœ ìºì‹œ íŒŒì¼ ê²½ë¡œ (Parquet í˜•ì‹)"""
        # ì‹¬ë³¼ ì •ê·œí™”: BTC/USDT:USDT â†’ btcusdt
        clean_symbol = symbol.replace('/', '').replace(':', '').lower()
        filename = f"{exchange.lower()}_{clean_symbol}_{timeframe}.parquet"
        return self.cache_dir / filename
    
    def download(self, symbol: str, timeframe: str, 
                 start_date: str = None, end_date: str = None,
                 exchange: str = "bybit", limit: int = 1000,
                 progress_callback=None, processor=None) -> pd.DataFrame:
        """
        ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ì €ì¥
        
        Args:
            symbol: 'BTCUSDT' or 'BTC/USDT:USDT'
            timeframe: '15m', '1h', '4h', '1d' ë“±
            start_date: '2024-01-01' (ì—†ìœ¼ë©´ limit ê°œìˆ˜ë§Œí¼)
            end_date: '2024-12-31' (ì—†ìœ¼ë©´ í˜„ì¬)
            exchange: ê±°ë˜ì†Œëª…
            limit: ìµœëŒ€ ìº”ë“¤ ìˆ˜
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
            processor: ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ í•¨ìˆ˜ (ì˜ˆ: ì§€í‘œ ì¶”ê°€)
        """
        cache_path = self._get_cache_path(exchange, symbol, timeframe)
        
        # 1. ê¸°ì¡´ ìºì‹œ í™•ì¸
        existing_df = self._load_cache(cache_path)
        
        # 2. ë‹¤ìš´ë¡œë“œ ë²”ìœ„ ê²°ì •
        if existing_df is not None and len(existing_df) > 0:
            last_time = existing_df['timestamp'].max()
            start_ts = int(last_time) + 1
            print(f"ğŸ“¦ ìºì‹œ ë°œê²¬: {len(existing_df)}ê°œ, ì´í›„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        else:
            if start_date:
                start_ts = int(pd.Timestamp(start_date).timestamp() * 1000)
            else:
                start_ts = None
            existing_df = pd.DataFrame()
        
        # 3. ìƒˆ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        new_data = self._fetch_ohlcv(
            symbol, timeframe, exchange, 
            since=start_ts, limit=limit,
            progress_callback=progress_callback
        )
        
        if new_data is None or len(new_data) == 0:
            print("ğŸ“­ ìƒˆ ë°ì´í„° ì—†ìŒ")
            return existing_df
        
        new_df = pd.DataFrame(new_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        
        # 4. ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        if len(existing_df) > 0:
            combined = pd.concat([existing_df, new_df], ignore_index=True)
            combined = combined.drop_duplicates(subset=['timestamp'], keep='last')
            combined = combined.sort_values('timestamp').reset_index(drop=True)
        else:
            combined = new_df
        
        # 4.5. ë°ì´í„° ê°€ê³µ (ì§€í‘œ ì¶”ê°€ ë“±)
        if processor:
            try:
                print("âš™ï¸ ë°ì´í„° ê°€ê³µ ì¤‘...")
                combined = processor(combined)
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° ê°€ê³µ ì‹¤íŒ¨: {e}")

        # 5. ìºì‹œ ì €ì¥
        self._save_cache(cache_path, combined)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {len(combined)}ê°œ ìº”ë“¤ â†’ {cache_path.name}")
        
        # [NEW] ë¹—ì¸-ì—…ë¹„íŠ¸ í•˜ì´í¬ë¦¬ë“œ: ì—…ë¹„íŠ¸ ë°ì´í„°ë¥¼ ë¹—ì¸ íŒŒì¼ë¡œ ë™ì‹œ ë³µì‚¬
        # ì—…ë¹„íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹œ ë¹—ì¸ íŒŒì¼ë„ ê°±ì‹ , ë¹—ì¸ ë‹¤ìš´ë¡œë“œ(ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨) ì‹œì—ë„ ë¹—ì¸ íŒŒì¼ ì €ì¥
        try:
            try:
                from GUI.constants import COMMON_KRW_SYMBOLS
            except ImportError:
                from constants import COMMON_KRW_SYMBOLS
            
            coin = symbol.split('/')[0].replace('KRW', '').replace('-', '').upper()
            if coin in COMMON_KRW_SYMBOLS:
                if exchange.lower() == 'upbit':
                    bithumb_cache = self._get_cache_path('bithumb', coin, timeframe)
                    self._save_cache(bithumb_cache, combined)
                    print(f"ğŸ”„ [HYBRID] Upbit data copied to Bithumb cache: {bithumb_cache.name}")
                elif exchange.lower() == 'bithumb':
                    upbit_cache = self._get_cache_path('upbit', coin, timeframe)
                    self._save_cache(upbit_cache, combined)
                    print(f"ğŸ”„ [HYBRID] Bithumb(Redirected) data copied to Upbit cache: {upbit_cache.name}")
        except Exception as e:
            print(f"âš ï¸ [HYBRID] Dual-saving failed: {e}")
            
        return combined
    
    # ì£¼ìš” ì½”ì¸ ìƒì¥ì¼ (í´ë°±ìš© - SymbolCache ì—†ì„ ë•Œ)
    COIN_LISTING_DATES = {
        'BTCUSDT': '2018-11-01',
        'ETHUSDT': '2018-11-01',
        'XRPUSDT': '2019-12-01',
        'SOLUSDT': '2021-06-01',
        'DOGEUSDT': '2021-06-02',
        'ADAUSDT': '2021-03-01',
        'AVAXUSDT': '2021-09-01',
        'DOTUSDT': '2021-01-01',
        'MATICUSDT': '2021-05-01',
        'LINKUSDT': '2020-08-01',
        'LTCUSDT': '2019-06-01',
        'ATOMUSDT': '2021-03-01',
        'UNIUSDT': '2021-02-01',
        'ETCUSDT': '2021-07-01',
        'APTUSDT': '2022-10-01',
        'ARBUSDT': '2023-03-01',
        'OPUSDT': '2022-06-01',
        'SUIUSDT': '2023-05-01',
        'NEARUSDT': '2021-10-01',
        'FILUSDT': '2021-04-01',
        'BNBUSDT': '2020-02-01',
    }
    
    def _get_listing_date(self, symbol: str, exchange: str = 'bybit') -> str:
        """ì½”ì¸ ìƒì¥ì¼ ë°˜í™˜ (SymbolCache ìš°ì„ , í´ë°±ìœ¼ë¡œ í•˜ë“œì½”ë”©)"""
        clean = symbol.replace('/', '').replace(':', '').upper()
        
        # 1. SymbolCacheì—ì„œ ì¡°íšŒ
        try:
            from symbol_cache import get_symbol_cache
            cache = get_symbol_cache()
            
            # ccxt í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¡°íšŒ
            ccxt_symbol = f"{clean[:-4]}/{clean[-4:]}:{clean[-4:]}"
            listing = cache.get_listing_date(exchange, ccxt_symbol)
            if listing:
                return listing
        except Exception:
            pass
        
        # 2. í•˜ë“œì½”ë”©ëœ ê°’ì—ì„œ ì¡°íšŒ
        if clean in self.COIN_LISTING_DATES:
            return self.COIN_LISTING_DATES[clean]
        
        # 3. ìƒì¥ì¼ ëª¨ë¥´ë©´ None ë°˜í™˜ -> _fetch_ohlcvì—ì„œ 2017ë…„ìœ¼ë¡œ ì²˜ë¦¬
        return None
    
    # ê±°ë˜ì†Œë³„ ìº”ë“¤ ìš”ì²­ ì œí•œ (ì•ˆì „í•œ ê°’)
    EXCHANGE_LIMITS = {
        'bithumb': 200,   # ë¹—ì¸ì€ 200ê°œê°€ ê°€ì¥ ì•ˆì •ì ì„
        'upbit': 1000,    # ì—…ë¹„íŠ¸ëŠ” ìµœëŒ€ 1000ê°œê¹Œì§€ ì§€ì› (ì¼ë¶€ ìº”ë“¤)
        'binance': 1000,
        'bybit': 1000,
        'okx': 100,       # OKXëŠ” í˜ì´ì§€ë‹¹ ì œí•œì´ ì—„ê²©í•¨
        'bitget': 1000,
        'bingx': 1000,
    }

    def _fetch_ohlcv(self, symbol: str, timeframe: str, exchange: str,
                     since: int = None, limit: int = 1000,
                     progress_callback=None) -> List:
        """OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ccxt ì‚¬ìš©)"""
        try:
            import ccxt
            
            exchange_id = exchange.lower()
            
            # [NEW] ë¹—ì¸-ì—…ë¹„íŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë‹¤ì´ë ‰ì…˜
            # ë¹—ì¸ì˜ ê²½ìš° ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë¯€ë¡œ, ì—…ë¹„íŠ¸ ê³µí†µ ì½”ì¸ì€ ì—…ë¹„íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
            if exchange_id == 'bithumb':
                try:
                    try:
                        from GUI.constants import COMMON_KRW_SYMBOLS
                    except ImportError:
                        from constants import COMMON_KRW_SYMBOLS
                    # ì‹¬ë³¼ì—ì„œ ì½”ì¸ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: BTC/KRW -> BTC, BTC -> BTC)
                    coin = symbol.split('/')[0].replace('KRW', '').replace('-', '').upper()
                    if coin in COMMON_KRW_SYMBOLS:
                        print(f"ğŸ”„ [HYBRID] Bithumb {coin} -> Switching to Upbit Data Source")
                        exchange_id = 'upbit'
                        # ì—…ë¹„íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‹¬ë³¼ ë³€í™˜
                        symbol = f"{coin}/KRW"
                except Exception as e:
                    print(f"âš ï¸ [HYBRID] Redirection failed: {e}")

            # [FIX] KRW ê±°ë˜ì†Œ ì²´í¬
            is_krw_exchange = exchange_id in ['bithumb', 'upbit']
            
            # ê±°ë˜ì†Œ ì¸ìŠ¤í„´ìŠ¤
            exchange_class = getattr(ccxt, exchange_id)
            ex = exchange_class({
                'enableRateLimit': True,
                'options': {'defaultType': 'spot' if is_krw_exchange else 'swap', 'adjustForTimeDifference': True},
                'timeout': 30000
            })
            ex.load_markets()
            
            # ì‹¬ë³¼ ë³€í™˜
            original_symbol = symbol
            if '/' not in symbol:
                if is_krw_exchange:
                    # [FIX] KRW-BTC â†’ BTC/KRW (ë¹—ì¸/ì—…ë¹„íŠ¸)
                    if symbol.startswith('KRW-'):
                        base = symbol.replace('KRW-', '')
                        symbol = f"{base}/KRW"
                    else:
                        # BTCKRW â†’ BTC/KRW
                        if symbol.endswith('KRW'):
                            symbol = f"{symbol[:-3]}/KRW"
                        else:
                            # ê¸°ë³¸ì ìœ¼ë¡œ BTC/KRW í˜•ì‹ìœ¼ë¡œ ì‹œë„
                            symbol = f"{symbol}/KRW"
                else:
                    # BTCUSDT â†’ BTC/USDT:USDT
                    symbol = f"{symbol[:-4]}/{symbol[-4:]}:{symbol[-4:]}"
            
            # ìƒì¥ì¼ ì²´í¬
            listing_date = self._get_listing_date(original_symbol, exchange_id)
            listing_ts = None
            if listing_date:
                try:
                    listing_ts = int(pd.Timestamp(listing_date).timestamp() * 1000)
                except: pass
            
            # since ì„¤ì •
            if since is None:
                if listing_ts:
                    since = listing_ts
                else:
                    # [FIX] ìƒì¥ì¼ ë¶ˆëª…í™• ì‹œ 2017.01.01ë¶€í„° ì „ì²´ ìˆ˜ì§‘ (ê¸°ì¡´ 2ë…„ ì œí•œ í•´ì œ)
                    since = int(pd.Timestamp("2017-01-01").timestamp() * 1000)
            elif listing_ts and since < listing_ts:
                since = listing_ts
            
            all_data = []
            fetched = 0
            
            # [FIX] ê±°ë˜ì†Œë³„ ë§ì¶¤í˜• ë°°ì¹˜ ì‚¬ì´ì¦ˆ
            batch_size = self.EXCHANGE_LIMITS.get(exchange_id, 1000)
            # ìš”ì²­í•œ limitë³´ë‹¤ batch_sizeê°€ í¬ë©´ ì¡°ì ˆ
            batch_size = min(batch_size, limit)
            
            print(f"ğŸ“¥ {exchange_id} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {symbol} ({timeframe}), Target: {limit}, Batch: {batch_size}")
            
            retry_count = 0
            max_retries = 3
            
            while fetched < limit:
                try:
                    # ë‚¨ì€ ê°œìˆ˜ê°€ batch_sizeë³´ë‹¤ ì‘ìœ¼ë©´ ì¡°ì ˆ
                    current_batch = min(batch_size, limit - fetched)
                    
                    data = ex.fetch_ohlcv(symbol, timeframe, since=since, limit=current_batch)
                    retry_count = 0
                except Exception as e:
                    retry_count += 1
                    if retry_count >= max_retries:
                        print(f"âŒ {exchange_id} ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                        break
                    print(f"âš ï¸ ì¬ì‹œë„ {retry_count}/{max_retries}...")
                    time.sleep(2)
                    continue
                
                if not data:
                    # ë°ì´í„°ê°€ ì—†ëŠ”ë° ì²˜ìŒì´ë©´ ìµœê·¼ ë°ì´í„°ë¡œ ì¬ì‹œë„
                    if fetched == 0 and since is not None:
                        print(f"â„¹ï¸ {exchange_id}: ê³¼ê±° ë°ì´í„° ì—†ìŒ, ìµœê·¼ë¶€í„° ì¬ì‹œë„")
                        since = None
                        continue
                    break
                
                # ì¤‘ë³µ ë°ì´í„° ì²´í¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                if all_data and data[0][0] == all_data[-1][0]:
                    print("â„¹ï¸ ì¤‘ë³µ ë°ì´í„° ë„ì°©, ìˆ˜ì§‘ ì¢…ë£Œ")
                    break
                    
                all_data.extend(data)
                fetched += len(data)
                
                if progress_callback:
                    progress_callback(fetched)
                
                # ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•œ since ì—…ë°ì´íŠ¸
                last_ts = data[-1][0]
                
                # í˜„ì¬ ì‹œê°„ì— ë„ë‹¬í–ˆëŠ”ì§€ ì²´í¬
                now_ts = int(time.time() * 1000)
                if last_ts >= now_ts - 60000: # 1ë¶„ ì´ë‚´
                    break
                    
                since = last_ts + 1
                time.sleep(0.3) # ë¶€í•˜ ì¡°ì ˆ
            
            # ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ìƒí™© ëŒ€ë¹„)
            if all_data:
                df_temp = pd.DataFrame(all_data, columns=['ts', 'o', 'h', 'l', 'c', 'v'])
                df_temp = df_temp.drop_duplicates(subset=['ts']).sort_values('ts')
                all_data = df_temp.values.tolist()
                
            print(f"âœ… {exchange_id} ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_data)}ê°œ ìº”ë“¤")
            return all_data
            
        except Exception as e:
            print(f"âŒ {symbol} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _load_cache(self, cache_path: Path) -> Optional[pd.DataFrame]:
        """Parquet ìºì‹œ ë¡œë“œ (ë ˆê±°ì‹œ SQLite í˜¸í™˜)"""
        # 1. Parquet íŒŒì¼ í™•ì¸
        if cache_path.exists():
            try:
                return pd.read_parquet(cache_path)
            except Exception as e:
                print(f"âš ï¸ Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 2. ë ˆê±°ì‹œ SQLite íŒŒì¼ í™•ì¸ (ë§ˆì´ê·¸ë ˆì´ì…˜ ì§€ì›)
        legacy_path = cache_path.with_suffix('.db')
        if legacy_path.exists():
            try:
                conn = sqlite3.connect(legacy_path)
                df = pd.read_sql("SELECT * FROM ohlcv ORDER BY timestamp", conn)
                conn.close()
                print(f"ğŸ“¦ ë ˆê±°ì‹œ DB â†’ Parquet ë³€í™˜: {legacy_path.name}")
                # ìë™ ë³€í™˜
                self._save_cache(cache_path, df)
                return df
            except Exception:
                pass  # ë ˆê±°ì‹œ DB ë³€í™˜ ì‹¤íŒ¨ ë¬´ì‹œ
        
        return None
    
    def _save_cache(self, cache_path: Path, df: pd.DataFrame):
        """Parquet ìºì‹œ ì €ì¥ (ì••ì¶•)"""
        df.to_parquet(cache_path, index=False, compression='snappy')
        print(f"ğŸ’¾ Parquet ì €ì¥: {cache_path.name} ({len(df):,}í–‰)")
    
    def load(self, symbol: str, timeframe: str, exchange: str = "bybit",
             start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ DataFrame)"""
        cache_path = self._get_cache_path(exchange, symbol, timeframe)
        df = self._load_cache(cache_path)
        
        if df is None:
            return pd.DataFrame()
        
        # ë‚ ì§œ í•„í„°ë§
        if start_date:
            start_ts = pd.Timestamp(start_date).timestamp() * 1000
            df = df[df['timestamp'] >= start_ts]
        if end_date:
            end_ts = pd.Timestamp(end_date).timestamp() * 1000
            df = df[df['timestamp'] <= end_ts]
        
        return df
    
    def load_data(self, symbol: str, exchange_id: str, timeframe: str,
                  start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """backtest_widget í˜¸í™˜ìš© load_data ë©”ì„œë“œ
        
        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'BTC/USDT:USDT' ë˜ëŠ” 'BTCUSDT')
            exchange_id: ê±°ë˜ì†Œ ID (ì˜ˆ: 'binance', 'bybit')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '15m', '1h')
            start_date: ì‹œì‘ì¼ (ì˜ˆ: '2024-01-01')
            end_date: ì¢…ë£Œì¼
        """
        # ì‹¬ë³¼ ì •ê·œí™” (BTC/USDT:USDT â†’ btcusdt, BTCUSDT â†’ btcusdt)
        normalized_symbol = symbol.replace('/', '').replace(':', '').lower()
        
        return self.load(
            symbol=normalized_symbol,
            timeframe=timeframe,
            exchange=exchange_id.lower(),
            start_date=start_date,
            end_date=end_date
        )
    

    def _get_db_metadata(self, db_path: Path):
        """DB ë©”íƒ€ë°ì´í„°ë§Œ ë¹ ë¥´ê²Œ ì¡°íšŒ (Parquet)"""
        try:
            # Parquet íŒŒì¼ì—ì„œ timestamp ì»¬ëŸ¼ë§Œ ì½ì–´ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            df = pd.read_parquet(db_path, columns=['timestamp'])
            if not df.empty:
                min_ts = df['timestamp'].min()
                max_ts = df['timestamp'].max()
                count = len(df)
                return min_ts, max_ts, count
            return None, None, 0
        except Exception as e:
            # ì½ê¸° ì‹¤íŒ¨ ì‹œ
            return None, None, 0

    def get_cache_list(self) -> List[CacheInfo]:
        """ìºì‹œëœ ë°ì´í„° ëª©ë¡ (ìµœì í™”ë¨)"""
        cache_list = []
        
        for db_file in self.cache_dir.glob("*.parquet"):
            try:
                parts = db_file.stem.split('_')
                if len(parts) >= 3:
                    exchange = parts[0]
                    symbol = parts[1].upper()
                    timeframe = parts[-1]
                    
                    # íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´(0ë°”ì´íŠ¸ ë“±) ìŠ¤í‚µ
                    if db_file.stat().st_size < 1024:
                        continue
                        
                    # ìµœì í™”: ì „ì²´ ë¡œë“œ ëŒ€ì‹  ë©”íƒ€ë°ì´í„°ë§Œ ì¡°íšŒ
                    min_ts, max_ts, count = self._get_db_metadata(db_file)
                    
                    if count > 0 and min_ts and max_ts:
                        cache_list.append(CacheInfo(
                            symbol=symbol,
                            timeframe=timeframe,
                            exchange=exchange,
                            start_date=datetime.utcfromtimestamp(min_ts / 1000),
                            end_date=datetime.utcfromtimestamp(max_ts / 1000),
                            candle_count=count,
                            file_size=db_file.stat().st_size
                        ))
            except Exception:
                continue
        
        return cache_list
    
    def delete_cache(self, exchange: str, symbol: str, timeframe: str) -> bool:
        """ìºì‹œ ì‚­ì œ"""
        cache_path = self._get_cache_path(exchange, symbol, timeframe)
        if cache_path.exists():
            cache_path.unlink()
            return True
        return False
    
    def cleanup_duplicates(self, dry_run: bool = True) -> Dict:
        """ì¤‘ë³µ ìºì‹œ ì •ë¦¬"""
        from collections import defaultdict
        
        # ì •ê·œí™”ëœ í‚¤ë¡œ ê·¸ë£¹í•‘
        groups = defaultdict(list)
        for db_file in self.cache_dir.glob("*.parquet"):
            parts = db_file.stem.lower().split('_')
            if len(parts) >= 3:
                key = f"{parts[0]}_{parts[1]}_{parts[-1]}"
                groups[key].append(db_file)
        
        # ì¤‘ë³µ ì°¾ê¸°
        duplicates = {k: v for k, v in groups.items() if len(v) > 1}
        
        result = {'found': len(duplicates), 'deleted': 0, 'kept': []}
        
        for key, files in duplicates.items():
            # ê°€ì¥ í° íŒŒì¼ ìœ ì§€
            files.sort(key=lambda f: f.stat().st_size, reverse=True)
            keep = files[0]
            delete = files[1:]
            
            result['kept'].append(keep.name)
            
            if not dry_run:
                for f in delete:
                    f.unlink()
                    result['deleted'] += 1
        
        return result
    
    def get_all_cache_list(self):
        """cache_manager_widgetê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        cache_list = self.get_cache_list()
        result = []
        
        for cache in cache_list:
            result.append({
                'exchange': cache.exchange,
                'symbol': cache.symbol,
                'timeframe': cache.timeframe,
                'first_date': cache.start_date.strftime('%Y-%m-%d'),
                'last_date': cache.end_date.strftime('%Y-%m-%d'),
                'count': cache.candle_count,
                'file_size': cache.file_size / (1024 * 1024),  # MB
                'filename': f"{cache.exchange}_{cache.symbol.lower()}_{cache.timeframe}.parquet"
            })
        
        return result
    
    # cache_manager_widgetê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±
    @property
    def CACHE_DIR(self):
        return self.cache_dir


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    dm = DataManager()
    
    print("ğŸ“‚ ìºì‹œ ë””ë ‰í† ë¦¬:", dm.cache_dir)
    print("ğŸ“Š ì§€ì› íƒ€ì„í”„ë ˆì„:", dm.TIMEFRAMES)
    
    # ìºì‹œ ëª©ë¡
    cache_list = dm.get_cache_list()
    print(f"\nğŸ“¦ ìºì‹œëœ ë°ì´í„°: {len(cache_list)}ê°œ")
    for c in cache_list[:5]:
        print(f"  - {c.exchange} {c.symbol} {c.timeframe}: {c.candle_count}ê°œ")
    
    # ì¤‘ë³µ ì²´í¬
    dup_result = dm.cleanup_duplicates(dry_run=True)
    print(f"\nğŸ” ì¤‘ë³µ ìºì‹œ: {dup_result['found']}ê°œ ê·¸ë£¹")
    
    # ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ (API ì—†ì´)
    print("\nğŸ“¥ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    df = dm.download('BTCUSDT', '15m', exchange='bybit', limit=100)
    if len(df) > 0:
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {len(df)}ê°œ ìº”ë“¤")
        print(df.tail(3))
