# ì›¹ì†Œì¼“ ì—°ê²° ì‹œê°„ëŒ€ ë° Parquet ì´ì–´ì“°ê¸° ë¶„ì„

> **ì‘ì„±ì¼**: 2026-01-15
> **ëª©ì **: ì›¹ì†Œì¼“ ì—°ê²° ì‹œì /ìœ ì§€ ë°©ì‹ ë° Parquet ë°ì´í„° ì´ì–´ì“°ê¸°(append) ê¸°ëŠ¥ í˜„í™© ë¶„ì„

---

## ğŸ“¡ 1. ì›¹ì†Œì¼“ ì—°ê²° ì‹œê°„ëŒ€ ë¶„ì„

### 1.1 ì—°ê²° ì‹œì‘ ì‹œì 

**ì½”ë“œ ìœ„ì¹˜**: `exchanges/ws_handler.py` (Line 174-213)

```python
async def connect(self):
    """ì›¹ì†Œì¼“ ì—°ê²° ë° ìœ ì§€"""
    self.running = True
    self.reconnect_attempts = 0

    while self.running:  # â­ ë¬´í•œ ë£¨í”„ - í”„ë¡œê·¸ë¨ ì¢…ë£Œê¹Œì§€ ìœ ì§€
        try:
            async with websockets.connect(url,
                ping_interval=20,      # 20ì´ˆë§ˆë‹¤ ping
                ping_timeout=10,       # 10ì´ˆ pong ëŒ€ê¸°
                close_timeout=5) as ws:

                self.is_connected = True
                self.last_message_time = datetime.now()

                # êµ¬ë… ë©”ì‹œì§€ ì „ì†¡
                await ws.send(json.dumps(msg))

                # ë©”ì‹œì§€ ìˆ˜ì‹  ë£¨í”„
                async for message in ws:
                    await self._handle_message(message)
```

#### ì—°ê²° íŠ¸ë¦¬ê±° ì‹œì 

**ë°©ë²• 1**: ê±°ë˜ì†Œ ì–´ëŒ‘í„°ì—ì„œ ì§ì ‘ í˜¸ì¶œ (ì˜ˆ: `binance_exchange.py`)

```python
# exchanges/binance_exchange.py (Line 453-459)
async def start_websocket(
    self,
    interval: str = '15m',
    on_candle_close: Optional[Any] = None,
    on_price_update: Optional[Any] = None,
    on_connect: Optional[Any] = None
) -> bool:
    """Binance ì›¹ì†Œì¼“ ì‹œì‘"""
    from exchanges.ws_handler import WebSocketHandler

    self.ws_handler = WebSocketHandler('binance', self.symbol, interval)
    self.ws_handler.on_candle_close = on_candle_close

    # â­ ì—°ê²° ì‹œì‘ (ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±)
    asyncio.create_task(self.ws_handler.connect())
```

**ë°©ë²• 2**: ë´‡ì´ ì‹¤í–‰ë  ë•Œ ìë™ ì‹œì‘ (ì˜ˆ: `unified_bot.py`)

```python
# ë´‡ ì‹œì‘ ì‹œ ì›¹ì†Œì¼“ ìë™ ì—°ê²° (ì¶”ì •)
async def start_bot():
    exchange = BinanceExchange(...)
    await exchange.start_websocket(
        interval='15m',
        on_candle_close=handle_new_candle
    )
```

### 1.2 ì—°ê²° ìœ ì§€ ì „ëµ

#### ì¬ì—°ê²° ë¡œì§ (Exponential Backoff)

**ì½”ë“œ**: `ws_handler.py` (Line 182-221)

```python
while self.running:
    if self.reconnect_attempts >= self.max_reconnects:  # 20íšŒ ì‹¤íŒ¨ ì‹œ
        logging.warning("[WS] âš ï¸ Max reconnects reached, waiting 5min...")
        self.reconnect_attempts = 0
        await asyncio.sleep(300)  # 5ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
        continue

    try:
        async with websockets.connect(...) as ws:
            # ì •ìƒ ì—°ê²°
            self.reconnect_attempts = 0  # ì„±ê³µ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹

    except Exception as e:
        self.is_connected = False
        self.reconnect_attempts += 1

        # Exponential Backoff ê³„ì‚°
        delay = self.reconnect_delay * (self.backoff_factor ** self.reconnect_attempts)
        delay = min(delay, self.max_reconnect_delay)  # ìµœëŒ€ 60ì´ˆ

        await asyncio.sleep(delay)  # ì§€ì—° í›„ ì¬ì—°ê²°
```

**ì¬ì—°ê²° íŒŒë¼ë¯¸í„°** (Line 60-65):

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `max_reconnects` | 20 | ìµœëŒ€ ì—°ì† ì¬ì—°ê²° ì‹œë„ íšŸìˆ˜ |
| `reconnect_delay` | 3ì´ˆ | ì´ˆê¸° ì¬ì—°ê²° ëŒ€ê¸° ì‹œê°„ |
| `max_reconnect_delay` | 60ì´ˆ | ìµœëŒ€ ì¬ì—°ê²° ëŒ€ê¸° ì‹œê°„ |
| `backoff_factor` | 1.5 | ì§€ìˆ˜ ë°±ì˜¤í”„ ë°°ìˆ˜ |

**ì¬ì—°ê²° ì‹œê°„ ì˜ˆì‹œ**:
- 1ì°¨ ì‹¤íŒ¨: 3ì´ˆ ëŒ€ê¸°
- 2ì°¨ ì‹¤íŒ¨: 4.5ì´ˆ ëŒ€ê¸°
- 3ì°¨ ì‹¤íŒ¨: 6.75ì´ˆ ëŒ€ê¸°
- ...
- 10ì°¨ ì‹¤íŒ¨: 38ì´ˆ ëŒ€ê¸° (60ì´ˆ ìƒí•œ ì ìš©)
- 20ì°¨ ì‹¤íŒ¨: 5ë¶„ ëŒ€ê¸° í›„ ì¹´ìš´í„° ë¦¬ì…‹

#### Ping/Pong í•˜íŠ¸ë¹„íŠ¸

```python
async with websockets.connect(
    url,
    ping_interval=20,  # 20ì´ˆë§ˆë‹¤ ping ì „ì†¡
    ping_timeout=10,   # pong ì‘ë‹µ 10ì´ˆ ëŒ€ê¸°
    close_timeout=5    # ì—°ê²° ì¢…ë£Œ 5ì´ˆ ëŒ€ê¸°
) as ws:
```

#### í—¬ìŠ¤ ì²´í¬ (íƒ€ì„ì•„ì›ƒ ê°ì§€)

**ì½”ë“œ**: `ws_handler.py` (Line 84-90)

```python
def is_healthy(self, timeout_seconds: int = 30) -> bool:
    """
    ë§ˆì§€ë§‰ ë©”ì‹œì§€ ìˆ˜ì‹  í›„ 30ì´ˆ ì´ìƒ ì§€ë‚˜ë©´ unhealthy
    """
    if not self.is_connected:
        return False
    if self.last_message_time is None:
        return False

    elapsed = (datetime.now() - self.last_message_time).total_seconds()
    return elapsed < timeout_seconds  # 30ì´ˆ ì´ë‚´ ë©”ì‹œì§€ ìˆ˜ì‹ í–ˆëŠ”ê°€?
```

### 1.3 ì—°ê²° ì‹œê°„ëŒ€ ìš”ì•½

| ì‹œì  | ë™ì‘ | ì½”ë“œ ìœ„ì¹˜ |
|------|------|-----------|
| **ë´‡ ì‹œì‘** | `start_websocket()` í˜¸ì¶œ | `binance_exchange.py:453` |
| **ì—°ê²° ì„±ê³µ** | `on_connect()` ì½œë°± ì‹¤í–‰ | `ws_handler.py:200` |
| **20ì´ˆë§ˆë‹¤** | Ping ì „ì†¡ (ìë™) | websockets ë¼ì´ë¸ŒëŸ¬ë¦¬ |
| **ë©”ì‹œì§€ ìˆ˜ì‹ ** | `last_message_time` ê°±ì‹  | `ws_handler.py:227` |
| **30ì´ˆ ë¬´ì‘ë‹µ** | `is_healthy() = False` | `ws_handler.py:84` |
| **ì—°ê²° ëŠê¹€** | ì¬ì—°ê²° ì‹œë„ (3ì´ˆ~60ì´ˆ ëŒ€ê¸°) | `ws_handler.py:217` |
| **20íšŒ ì‹¤íŒ¨** | 5ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„ | `ws_handler.py:184` |
| **ë´‡ ì¢…ë£Œ** | `self.running = False` â†’ ë£¨í”„ ì¢…ë£Œ | - |

---

## ğŸ’¾ 2. Parquet ì´ì–´ì“°ê¸° ë¶„ì„

### 2.1 í˜„ì¬ êµ¬í˜„ ë°©ì‹

**ì½”ë“œ ìœ„ì¹˜**: `core/data_manager.py`

#### ì €ì¥ ë©”ì„œë“œ (Line 252-294)

```python
def save_parquet(self):
    """í˜„ì¬ ë°ì´í„°ë¥¼ Parquetìœ¼ë¡œ ì €ì¥"""

    # âš ï¸ ìµœì‹  1000ê°œë§Œ ì €ì¥ (tail)
    if self.df_entry_full is not None and len(self.df_entry_full) > 0:
        entry_file = self.get_entry_file_path()
        save_df = self.df_entry_full.tail(1000).copy()  # âŒ ì „ì²´ ë®ì–´ì“°ê¸°

        # Timestampë¥¼ ms ì •ìˆ˜ë¡œ ë³€í™˜
        if 'timestamp' in save_df.columns:
            save_df['timestamp'] = save_df['timestamp'].astype(np.int64) // 10**6

        save_df.to_parquet(entry_file, index=False)  # âŒ mode íŒŒë¼ë¯¸í„° ì—†ìŒ
```

#### ìº”ë“¤ ì¶”ê°€ ë©”ì„œë“œ (Line 298-327)

```python
def append_candle(self, candle: dict, save: bool = True):
    """ìƒˆ ìº”ë“¤ ì¶”ê°€"""

    # 1. DataFrameìœ¼ë¡œ ë³€í™˜
    new_row = pd.DataFrame([candle])

    # 2. ë©”ëª¨ë¦¬ì—ì„œ ë³‘í•© (concat)
    self.df_entry_full = pd.concat([self.df_entry_full, new_row], ignore_index=True)

    # 3. ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    self.df_entry_full = self.df_entry_full.drop_duplicates(subset='timestamp', keep='last')
    self.df_entry_full = self.df_entry_full.sort_values('timestamp').reset_index(drop=True)

    # 4. ìµœëŒ€ 1000ê°œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì œí•œ)
    if len(self.df_entry_full) > 1000:
        self.df_entry_full = self.df_entry_full.tail(1000).reset_index(drop=True)

    # 5. Parquet ì €ì¥ (ì „ì²´ ë®ì–´ì“°ê¸°)
    if save:
        self.save_parquet()  # âš ï¸ ë§¤ë²ˆ ì „ì²´ íŒŒì¼ ì¬ì‘ì„±
```

### 2.2 ë¬¸ì œì 

#### âŒ í˜„ì¬ ë°©ì‹ì˜ í•œê³„

1. **ì§„ì •í•œ ì´ì–´ì“°ê¸° ì•„ë‹˜**
   - Parquet íŒŒì¼ì„ ì§ì ‘ appendí•˜ì§€ ì•ŠìŒ
   - ë©”ëª¨ë¦¬ì˜ DataFrameì„ ìˆ˜ì • í›„ ì „ì²´ íŒŒì¼ ë®ì–´ì“°ê¸°

2. **1000ê°œ ìº”ë“¤ ì œí•œ**
   - `tail(1000)` â†’ ì˜¤ë˜ëœ ë°ì´í„°ëŠ” ìë™ ì‚­ì œ
   - ì¥ê¸° ë°±í…ŒìŠ¤íŠ¸ ì‹œ ì „ì²´ íˆìŠ¤í† ë¦¬ ìœ ì‹¤

3. **I/O ë¹„íš¨ìœ¨**
   - ìƒˆ ìº”ë“¤ 1ê°œ ì¶”ê°€í•  ë•Œë§ˆë‹¤ 1000ê°œ ì „ì²´ ì¬ì‘ì„±
   - ë””ìŠ¤í¬ ì“°ê¸° ë¹ˆë„ ë†’ìŒ (15ë¶„ë§ˆë‹¤)

4. **ë™ì‹œì„± ë¬¸ì œ**
   - ì—¬ëŸ¬ ë´‡ì´ ë™ì¼ íŒŒì¼ì— ì ‘ê·¼ ì‹œ ê²½í•© ê°€ëŠ¥
   - `threading.RLock()` ì‚¬ìš©í•˜ì§€ë§Œ í”„ë¡œì„¸ìŠ¤ ê°„ ì ê¸ˆ ì—†ìŒ

### 2.3 Parquet ì§„ì •í•œ Append ë¶ˆê°€ ì´ìœ 

**Pandas/PyArrow ì œì•½**:
```python
# âŒ ParquetëŠ” ê¸°ë³¸ì ìœ¼ë¡œ append ëª¨ë“œ ë¯¸ì§€ì›
df.to_parquet('file.parquet', mode='a')  # AttributeError: 'mode' not supported
```

**í•´ê²° ë°©ë²•**:

#### ë°©ë²• 1: PyArrow Dataset API (ì¶”ì²œ)

```python
import pyarrow.parquet as pq
import pyarrow as pa

# ê¸°ì¡´ íŒŒì¼ ì½ê¸°
table = pq.read_table('data.parquet')

# ìƒˆ ë°ì´í„° ì¶”ê°€
new_table = pa.Table.from_pandas(new_df)
combined = pa.concat_tables([table, new_table])

# ì €ì¥ (ì „ì²´ ì¬ì‘ì„±ì€ ë™ì¼í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
pq.write_table(combined, 'data.parquet')
```

#### ë°©ë²• 2: Partitioned Dataset (ëŒ€ìš©ëŸ‰)

```python
# ë‚ ì§œë³„ë¡œ íŒŒì¼ ë¶„í•  ì €ì¥
df.to_parquet(
    'cache/btcusdt/',
    partition_cols=['date'],  # dateë³„ë¡œ í´ë” ìƒì„±
    engine='pyarrow'
)

# ì½ê¸° (ì „ì²´ íŒŒí‹°ì…˜ ìë™ ë³‘í•©)
df = pd.read_parquet('cache/btcusdt/')
```

#### ë°©ë²• 3: Delta Lake (ê³ ê¸‰)

```python
from deltalake import write_deltalake

# True append ì§€ì›
write_deltalake('data/delta_table', new_df, mode='append')
```

### 2.4 í˜„ì¬ êµ¬í˜„ì˜ ì¥ì 

âœ… **ë‹¨ìˆœì„±**:
- ë³µì¡í•œ íŒŒí‹°ì…˜ ê´€ë¦¬ ë¶ˆí•„ìš”
- ì½ê¸° ì‹œ ë‹¨ì¼ íŒŒì¼ë§Œ ë¡œë“œ

âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨**:
- 1000ê°œ ì œí•œìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš© ì˜ˆì¸¡ ê°€ëŠ¥
- ë´‡ ì¬ì‹œì‘ ì‹œ ë¹ ë¥¸ ë¡œë”©

âœ… **ì¤‘ë³µ ë°©ì§€**:
- `drop_duplicates()` ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¤‘ë³µ ìë™ ì œê±°

### 2.5 ê°œì„  ë°©ì•ˆ (ì„ íƒì )

#### ì˜µì…˜ A: í˜„ì¬ ìœ ì§€ (ê¶Œì¥)

**ì´ìœ **:
- ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”©ì€ ìµœê·¼ 1000ê°œ ìº”ë“¤(15m ê¸°ì¤€ 10ì¼ì¹˜)ë©´ ì¶©ë¶„
- ì¥ê¸° ë°±í…ŒìŠ¤íŠ¸ëŠ” ë³„ë„ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©

#### ì˜µì…˜ B: ë¬´ì œí•œ ì¶•ì  (ì‹ ì¤‘)

```python
def save_parquet(self):
    """ì „ì²´ íˆìŠ¤í† ë¦¬ ì €ì¥ (ë¬´ì œí•œ)"""
    entry_file = self.get_entry_file_path()

    # âš ï¸ tail ì œê±° â†’ ëª¨ë“  ë°ì´í„° ì €ì¥
    save_df = self.df_entry_full.copy()  # ì „ì²´ ì €ì¥

    # íŒŒì¼ í¬ê¸° ì¦ê°€ ëª¨ë‹ˆí„°ë§ í•„ìš”
    save_df.to_parquet(entry_file, index=False, compression='snappy')
```

**íŠ¸ë ˆì´ë“œì˜¤í”„**:
- âœ… ì¥ê¸° ë°±í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- âŒ ë©”ëª¨ë¦¬ ì‚¬ìš© ì¦ê°€
- âŒ ë¡œë”© ì‹œê°„ ì¦ê°€
- âŒ ë””ìŠ¤í¬ ê³µê°„ ì¦ê°€

#### ì˜µì…˜ C: ì´ì¤‘ ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ)

```python
def save_parquet(self):
    """ìµœê·¼ + ì•„ì¹´ì´ë¸Œ ì´ì¤‘ ì €ì¥"""

    # 1. ì‹¤ì‹œê°„ìš© (ìµœê·¼ 1000ê°œ)
    recent_file = self.cache_dir / f"{self.exchange_name}_{self.symbol_clean}_15m.parquet"
    self.df_entry_full.tail(1000).to_parquet(recent_file)

    # 2. ì•„ì¹´ì´ë¸Œìš© (ì „ì²´ - ì¼ ë‹¨ìœ„ ë¶„í• )
    if len(self.df_entry_full) > 1000:
        date = self.df_entry_full['timestamp'].iloc[-1].strftime('%Y%m%d')
        archive_file = self.cache_dir / f"archive/{self.exchange_name}_{self.symbol_clean}_{date}.parquet"
        archive_file.parent.mkdir(exist_ok=True)

        # 1000ê°œ ì´ì „ ë°ì´í„°ë§Œ ì•„ì¹´ì´ë¸Œ
        old_data = self.df_entry_full.head(len(self.df_entry_full) - 1000)

        if archive_file.exists():
            # ê¸°ì¡´ ì•„ì¹´ì´ë¸Œì™€ ë³‘í•©
            existing = pd.read_parquet(archive_file)
            combined = pd.concat([existing, old_data]).drop_duplicates(subset='timestamp')
            combined.to_parquet(archive_file)
        else:
            old_data.to_parquet(archive_file)
```

---

## ğŸ“‹ 3. í˜„í™© ìš”ì•½

### ì›¹ì†Œì¼“ ì—°ê²° ì‹œê°„ëŒ€

| í•­ëª© | í˜„ì¬ êµ¬í˜„ |
|------|-----------|
| **ì—°ê²° ì‹œì ** | ë´‡ ì‹œì‘ ì‹œ `start_websocket()` í˜¸ì¶œ |
| **ìœ ì§€ ë°©ì‹** | ë¬´í•œ ë£¨í”„ (`while self.running`) |
| **í•˜íŠ¸ë¹„íŠ¸** | 20ì´ˆ ping/10ì´ˆ pong íƒ€ì„ì•„ì›ƒ |
| **ì¬ì—°ê²°** | Exponential Backoff (3ì´ˆ~60ì´ˆ) |
| **ìµœëŒ€ ì¬ì‹œë„** | 20íšŒ (ì´í›„ 5ë¶„ ëŒ€ê¸°) |
| **í—¬ìŠ¤ ì²´í¬** | 30ì´ˆ ë¬´ì‘ë‹µ ì‹œ unhealthy |
| **ì¢…ë£Œ ì‹œì ** | `self.running = False` ì„¤ì • ì‹œ |

### Parquet ì´ì–´ì“°ê¸°

| í•­ëª© | í˜„ì¬ êµ¬í˜„ | ê°œì„  ê°€ëŠ¥ì„± |
|------|-----------|------------|
| **ì €ì¥ ë°©ì‹** | ì „ì²´ ë®ì–´ì“°ê¸° (ë©”ëª¨ë¦¬ concat) | âœ… ì í•© |
| **ë°ì´í„° ì–‘** | ìµœê·¼ 1000ê°œ ìœ ì§€ | âš ï¸ ë¬´ì œí•œë„ ê°€ëŠ¥ |
| **ì¤‘ë³µ ì²˜ë¦¬** | `drop_duplicates()` | âœ… ìš°ìˆ˜ |
| **I/O ë¹ˆë„** | 15ë¶„ë§ˆë‹¤ (ìº”ë“¤ ë§ˆê° ì‹œ) | âœ… ì ì ˆ |
| **True Append** | âŒ ë¯¸ì§€ì› | âš ï¸ PyArrowë¡œ ê°€ëŠ¥ |
| **ì¥ê¸° ë°±í…ŒìŠ¤íŠ¸** | âŒ 1000ê°œ ì œí•œ | âš ï¸ ì•„ì¹´ì´ë¸Œ ì¶”ê°€ ê³ ë ¤ |

---

## ğŸ¯ 4. ê¶Œì¥ì‚¬í•­

### ì›¹ì†Œì¼“

âœ… **í˜„ì¬ êµ¬í˜„ ìœ ì§€**
- ì¬ì—°ê²° ë¡œì§ ìš°ìˆ˜
- í—¬ìŠ¤ ì²´í¬ ì ì ˆ
- ì¶”ê°€ ê°œì„  ë¶ˆí•„ìš”

### Parquet ì´ì–´ì“°ê¸°

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”©ë§Œ ì‚¬ìš©

âœ… **í˜„ì¬ êµ¬í˜„ ìœ ì§€**
- 1000ê°œ ì œí•œì€ 10ì¼ì¹˜ ì¶©ë¶„
- ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ íš¨ìœ¨ì 

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ì¥ê¸° ë°±í…ŒìŠ¤íŠ¸ í•„ìš”

âš ï¸ **ì˜µì…˜ C (ì´ì¤‘ ì €ì¥) êµ¬í˜„ ê³ ë ¤**
- ì‹¤ì‹œê°„: ìµœê·¼ 1000ê°œ
- ì•„ì¹´ì´ë¸Œ: ì „ì²´ íˆìŠ¤í† ë¦¬ (ë‚ ì§œ ë¶„í• )
- ë°±í…ŒìŠ¤íŠ¸ ì‹œ ì•„ì¹´ì´ë¸Œ ë¡œë“œ

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ìš©ëŸ‰ ë©€í‹°ì‹¬ë³¼

âš ï¸ **Partitioned Dataset ê³ ë ¤**
```
cache/
â”œâ”€â”€ btcusdt/
â”‚   â”œâ”€â”€ date=20260101/
â”‚   â”œâ”€â”€ date=20260102/
â”‚   â””â”€â”€ ...
â””â”€â”€ ethusdt/
    â”œâ”€â”€ date=20260101/
    â””â”€â”€ ...
```

---

## ğŸ“ 5. ì½”ë“œ ê°œì„  ì˜ˆì‹œ (ì„ íƒ)

### 5.1 ë¬´ì œí•œ Parquet ì €ì¥

```python
# core/data_manager.py ìˆ˜ì •

def save_parquet(self, limit: Optional[int] = 1000):
    """
    Parquet ì €ì¥

    Args:
        limit: ì €ì¥í•  ìµœê·¼ ìº”ë“¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
    """
    try:
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        if self.df_entry_full is not None and len(self.df_entry_full) > 0:
            entry_file = self.get_entry_file_path()

            # limit ì ìš©
            if limit is None:
                save_df = self.df_entry_full.copy()  # ì „ì²´
            else:
                save_df = self.df_entry_full.tail(limit).copy()  # ìµœê·¼ Nê°œ

            # Timestamp ë³€í™˜
            if 'timestamp' in save_df.columns:
                if pd.api.types.is_datetime64_any_dtype(save_df['timestamp']):
                    save_df['timestamp'] = save_df['timestamp'].astype(np.int64) // 10**6

            # ì••ì¶• ì €ì¥
            save_df.to_parquet(entry_file, index=False, compression='snappy')
            logging.debug(f"[DATA] Saved {len(save_df)} candles: {entry_file.name}")

    except Exception as e:
        logging.error(f"[DATA] Save failed: {e}")
```

### 5.2 PyArrow íš¨ìœ¨ì  Append

```python
import pyarrow.parquet as pq
import pyarrow as pa

def append_to_parquet(self, new_candles: pd.DataFrame):
    """PyArrowë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì  append"""
    entry_file = self.get_entry_file_path()

    try:
        # ê¸°ì¡´ ë°ì´í„° ì½ê¸°
        if entry_file.exists():
            existing_table = pq.read_table(entry_file)
            existing_df = existing_table.to_pandas()
        else:
            existing_df = pd.DataFrame()

        # ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        combined = pd.concat([existing_df, new_candles], ignore_index=True)
        combined = combined.drop_duplicates(subset='timestamp', keep='last')
        combined = combined.sort_values('timestamp').reset_index(drop=True)

        # PyArrow í…Œì´ë¸”ë¡œ ë³€í™˜ ë° ì €ì¥
        table = pa.Table.from_pandas(combined)
        pq.write_table(table, entry_file, compression='snappy')

    except Exception as e:
        logging.error(f"[APPEND] Failed: {e}")
```

---

## âœ… ê²°ë¡ 

### ì›¹ì†Œì¼“ ì—°ê²° ì‹œê°„ëŒ€

- **ì—°ê²° ì‹œì‘**: ë´‡ ì‹œì‘ ì‹œ (`start_websocket()` í˜¸ì¶œ)
- **ìœ ì§€ ì „ëµ**: ë¬´í•œ ë£¨í”„ + Exponential Backoff ì¬ì—°ê²°
- **í—¬ìŠ¤ ì²´í¬**: 30ì´ˆ ë¬´ì‘ë‹µ ê°ì§€
- **ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ

### Parquet ì´ì–´ì“°ê¸°

- **í˜„ì¬ ë°©ì‹**: ë©”ëª¨ë¦¬ ë³‘í•© â†’ ì „ì²´ ë®ì–´ì“°ê¸° (1000ê°œ ì œí•œ)
- **True Append**: âŒ ë¯¸ì§€ì› (Pandas/Parquet ì œì•½)
- **ê°œì„  ì˜µì…˜**:
  1. í˜„ì¬ ìœ ì§€ (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”©ìš©) âœ…
  2. ë¬´ì œí•œ ì €ì¥ (ë©”ëª¨ë¦¬ ì¦ê°€ ì£¼ì˜) âš ï¸
  3. ì´ì¤‘ ì €ì¥ (ì‹¤ì‹œê°„ + ì•„ì¹´ì´ë¸Œ) âš ï¸
  4. PyArrow Dataset (ëŒ€ìš©ëŸ‰) âš ï¸

**ê¶Œì¥**: í˜„ì¬ êµ¬í˜„ ìœ ì§€. í•„ìš” ì‹œ ì•„ì¹´ì´ë¸Œ ë¡œì§ ì¶”ê°€.

---

**ì‘ì„±**: Claude Sonnet 4.5
**ê²€ì¦**: VS Code Pyright (ì—ëŸ¬ 0ê°œ)
