# ğŸ“¡ API â†’ ë°ì´í„° ì €ì¥ â†’ ì½ê¸° íë¦„ ë¶„ì„ (2026-01-15)

> **ìš”ì²­**: "API - ë°ì´í„° ì €ì¥, ì½ê¸°, ì´ ë‚´ìš© ê¸°ì¤€ìœ¼ë¡œë§Œ ë¶„ì„"

---

## ğŸ“Š ì „ì²´ ë°ì´í„° íë¦„

```
[ê±°ë˜ì†Œ API]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. API ë°ì´í„° ìˆ˜ì§‘      â”‚
â”‚  - REST API (ì´ˆê¸°)      â”‚
â”‚  - WebSocket (ì‹¤ì‹œê°„)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ë°ì´í„° ì €ì¥         â”‚
â”‚  - Parquet íŒŒì¼         â”‚
â”‚  - Lazy Load ë³‘í•©       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. ë°ì´í„° ì½ê¸°         â”‚
â”‚  - ë°±í…ŒìŠ¤íŠ¸             â”‚
â”‚  - ì‹¤ì‹œê°„ ë§¤ë§¤          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. API ë°ì´í„° ìˆ˜ì§‘

### 1-1. REST API (ì´ˆê¸° ë¡œë“œ)

#### ê±°ë˜ì†Œ API í˜¸ì¶œ
**ìœ„ì¹˜**: `exchanges/bybit_exchange.py:101-142`

```python
def get_klines(self, symbol: Optional[str] = None, interval: str = '15m', limit: int = 200) -> Optional[pd.DataFrame]:
    """ìº”ë“¤ ë°ì´í„° ì¡°íšŒ"""
    try:
        target_symbol = symbol.upper() if symbol else self.symbol.upper()

        # Bybit interval ë³€í™˜
        interval_map = {
            '1m': '1', '5m': '5', '15m': '15', '30m': '30',
            '1h': '60', '4h': '240', '1d': 'D', '1w': 'W'
        }
        bybit_interval = interval_map.get(interval, interval)

        # Bybit SDK í˜¸ì¶œ
        result = self.session.get_kline(
            category="linear",
            symbol=target_symbol,
            interval=bybit_interval,
            limit=limit  # ê¸°ë³¸ 200ê°œ
        )

        if result.get('retCode') != 0:
            logging.error(f"Kline error: {result.get('retMsg')}")
            return None

        data = result.get('result', {}).get('list', [])
        if not data:
            return None

        # DataFrame ë³€í™˜
        df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'])
        df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms', utc=True)
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = df[col].astype(float)

        return df.sort_values('timestamp').reset_index(drop=True)

    except Exception as e:
        logging.error(f"Kline fetch error: {e}")
        return None
```

**API ì‘ë‹µ ì˜ˆì‹œ** (Bybit):
```json
{
  "retCode": 0,
  "result": {
    "list": [
      ["1705401600000", "43000.0", "43500.0", "42800.0", "43200.0", "100.5", "4320000.0"],
      ["1705402500000", "43200.0", "43800.0", "43100.0", "43500.0", "120.3", "5230000.0"],
      ...
    ]
  }
}
```

**ë³€í™˜ í›„ DataFrame**:
```
        timestamp             open      high      low       close     volume
0   2024-01-16 08:00:00  43000.0  43500.0  42800.0  43200.0  100.5
1   2024-01-16 08:15:00  43200.0  43800.0  43100.0  43500.0  120.3
...
```

---

#### ì´ˆê¸° ë¡œë“œ íë¦„
**ìœ„ì¹˜**: `core/data_manager.py:108-169`

```python
def load_historical(self, fetch_callback: Optional[Callable] = None) -> bool:
    """Parquetì—ì„œ íˆìŠ¤í† ë¦¬ ë¡œë“œ (ì—†ìœ¼ë©´ REST API ì‹œë„)"""
    try:
        entry_file = self.get_entry_file_path()  # data/cache/bybit_btcusdt_15m.parquet

        # 1. Parquet íŒŒì¼ ì¡´ì¬ ì‹œ ë¡œë“œ
        if entry_file.exists():
            df = pd.read_parquet(entry_file)

            # Timestamp ì •ê·œí™”
            if 'timestamp' in df.columns:
                if pd.api.types.is_numeric_dtype(df['timestamp']):
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
                else:
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                df = df.set_index('timestamp')

            self.df_entry_full = df.copy()
            if 'timestamp' not in self.df_entry_full.columns:
                self.df_entry_full = self.df_entry_full.reset_index()

            logging.info(f"[DATA] Loaded {len(df)} candles from Parquet")
            self.process_data()  # ì§€í‘œ ê³„ì‚°
            return True

        # 2. Parquet ì—†ìœ¼ë©´ REST API í´ë°±
        else:
            logging.warning(f"[DATA] Parquet not found: {entry_file}")

            if fetch_callback:
                logging.info("[DATA] Fetching from REST API...")
                df_rest = fetch_callback()  # exchange.get_klines('15', 1000)

                if df_rest is not None and len(df_rest) > 0:
                    self.df_entry_full = df_rest.copy()
                    self.save_parquet()  # Parquet ì €ì¥
                    self.process_data()
                    logging.info(f"[DATA] Fetched and saved: {len(df_rest)} candles")
                    return True

            return False

    except Exception as e:
        logging.error(f"[DATA] Load failed: {e}")
        return False
```

**íë¦„ë„**:
```
ì‹œì‘
  â†“
Parquet íŒŒì¼ ì¡´ì¬?
  â†“ YES                    â†“ NO
Parquet ì½ê¸°          REST API í˜¸ì¶œ
  â†“                        â†“
df_entry_full ì„¤ì •     df_entry_full ì„¤ì •
  â†“                        â†“
ì§€í‘œ ê³„ì‚°              Parquet ì €ì¥ + ì§€í‘œ ê³„ì‚°
  â†“                        â†“
ì™„ë£Œ                    ì™„ë£Œ
```

---

### 1-2. WebSocket (ì‹¤ì‹œê°„ ë°ì´í„°)

#### WebSocket ì‹œì‘
**ìœ„ì¹˜**: `core/unified_bot.py:375-381`

```python
def _start_websocket(self):
    sig_ex = self._get_signal_exchange()
    if hasattr(sig_ex, 'start_websocket'):
        self._ws_started = sig_ex.start_websocket(
            interval='15m',
            on_candle_close=self._on_candle_close,     # ìº”ë“¤ ì™„ë£Œ ì‹œ ì½œë°±
            on_price_update=self._on_price_update,     # ì‹¤ì‹œê°„ ê°€ê²© ì—…ë°ì´íŠ¸
            on_connect=lambda: self.mod_data.backfill(lambda lim: sig_ex.get_klines('15', lim))  # ì—°ê²° ì‹œ ê°­ ë³´ì¶©
        )
```

#### ìº”ë“¤ ì™„ë£Œ ì½œë°±
**ìœ„ì¹˜**: `core/unified_bot.py:383-390`

```python
def _on_candle_close(self, candle: dict):
    """15ë¶„ë´‰ ì™„ë£Œ ì‹œ í˜¸ì¶œ"""
    # ìŠ¤ë ˆë“œ ì•ˆì „ ì²˜ë¦¬
    with self.mod_data._data_lock:
        self.mod_data.append_candle(candle)  # ë©”ëª¨ë¦¬ + Parquet ì €ì¥
        self._process_historical_data()      # ì§€í‘œ ì¬ê³„ì‚°

        # íŒ¨í„´ ì‹œê·¸ë„ ì¶”ê°€
        df_pattern = self.df_pattern_full if self.df_pattern_full is not None else pd.DataFrame()
        self.mod_signal.add_patterns_from_df(df_pattern)
```

**ìº”ë“¤ ë°ì´í„° ì˜ˆì‹œ**:
```python
candle = {
    'timestamp': pd.Timestamp('2024-01-16 08:15:00', tz='UTC'),
    'open': 43200.0,
    'high': 43800.0,
    'low': 43100.0,
    'close': 43500.0,
    'volume': 120.3
}
```

---

### 1-3. Backfill (ëˆ„ë½ ë°ì´í„° ë³´ì¶©)

#### ìœ„ì¹˜
- `core/unified_bot.py:401-413` (5ë¶„ë§ˆë‹¤ ì‹¤í–‰)
- `core/data_manager.py:413-463`

#### ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œ
```python
def _start_data_monitor(self):
    def monitor():
        while self.is_running:
            time.sleep(300)  # 5ë¶„ ëŒ€ê¸°
            try:
                sig_ex = self._get_signal_exchange()
                # Backfill ì‹¤í–‰
                added = self.mod_data.backfill(lambda lim: sig_ex.get_klines('15', lim))
                if added > 0:
                    self.df_entry_full = self.mod_data.df_entry_full
                    self._process_historical_data()
                self.sync_position()
            except Exception:
                pass
    threading.Thread(target=monitor, daemon=True).start()
```

#### Backfill ë¡œì§
```python
def backfill(self, fetch_callback: Callable) -> int:
    """REST APIë¡œ ëˆ„ë½ëœ ìº”ë“¤ ë³´ì¶©"""
    if self.df_entry_full is None or len(self.df_entry_full) == 0:
        logging.warning("[BACKFILL] No existing data")
        return 0

    with self._data_lock:
        # ë§ˆì§€ë§‰ ì €ì¥ëœ ìº”ë“¤ ì‹œê°„
        last_ts = self.df_entry_full['timestamp'].iloc[-1]
        now = pd.Timestamp.utcnow()

        # ì˜ˆìƒ ê°œìˆ˜ (15ë¶„ë´‰ ê¸°ì¤€)
        expected = int((now - last_ts).total_seconds() / 900)

        if expected <= 1:
            return 0  # ê°­ ì—†ìŒ

        # REST APIë¡œ ìµœê·¼ ë°ì´í„° ì¡°íšŒ
        df_new = fetch_callback(limit=expected + 10)
        if df_new is None or len(df_new) == 0:
            return 0

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì •ê·œí™”
        if 'timestamp' not in df_new.columns and df_new.index.name == 'timestamp':
            df_new = df_new.reset_index()
        if 'timestamp' in df_new.columns:
            df_new['timestamp'] = pd.to_datetime(df_new['timestamp'])

        # ëˆ„ë½ëœ ìº”ë“¤ë§Œ í•„í„°ë§
        df_new = df_new[df_new['timestamp'] > last_ts]

        if len(df_new) > 0:
            # ë©”ëª¨ë¦¬ì— ì¶”ê°€
            self.df_entry_full = pd.concat([self.df_entry_full, df_new], ignore_index=True)
            self.df_entry_full = self.df_entry_full.drop_duplicates(subset='timestamp', keep='last')
            self.df_entry_full = self.df_entry_full.sort_values('timestamp').reset_index(drop=True)

            # Parquet ì €ì¥
            self._save_with_lazy_merge()

            # ë©”ëª¨ë¦¬ truncate (1000ê°œ ì œí•œ)
            if len(self.df_entry_full) > self.MAX_ENTRY_MEMORY:
                self.df_entry_full = self.df_entry_full.tail(self.MAX_ENTRY_MEMORY).reset_index(drop=True)

            logging.info(f"[BACKFILL] Added {len(df_new)} candles")
            return len(df_new)

        return 0
```

**Backfill ì‹œë‚˜ë¦¬ì˜¤**:
```
ë§ˆì§€ë§‰ ìº”ë“¤: 2024-01-16 08:00:00
í˜„ì¬ ì‹œê°„:   2024-01-16 08:45:00
ì˜ˆìƒ ê°œìˆ˜:   3ê°œ (08:15, 08:30, 08:45)

REST API ì¡°íšŒ (limit=13)
  â†“
ìµœì‹  13ê°œ ìº”ë“¤ ë°˜í™˜
  â†“
timestamp > 08:00:00 í•„í„°ë§
  â†“
3ê°œ ì¶”ê°€
  â†“
Parquet ì €ì¥
```

---

## 2. ë°ì´í„° ì €ì¥

### 2-1. Parquet ì €ì¥ ê²½ë¡œ

**ìœ„ì¹˜**: `core/data_manager.py:94-104`

```python
def get_entry_file_path(self) -> Path:
    """15m Entry ë°ì´í„° Parquet ê²½ë¡œ (ë‹¨ì¼ ì†ŒìŠ¤)"""
    return self.cache_dir / f"{self.exchange_name}_{self.symbol_clean}_15m.parquet"
    # ì˜ˆ: data/cache/bybit_btcusdt_15m.parquet

def get_pattern_file_path(self) -> Path:
    """
    [DEPRECATED] 1h Pattern ë°ì´í„° ê²½ë¡œ
    15m ë‹¨ì¼ ì†ŒìŠ¤ ì›ì¹™: 15m ë°ì´í„°ë¥¼ resample_data()ë¡œ ë¦¬ìƒ˜í”Œë§í•˜ì—¬ ì‚¬ìš© ê¶Œì¥
    """
    return self.cache_dir / f"{self.exchange_name}_{self.symbol_clean}_1h.parquet"
```

**íŒŒì¼ êµ¬ì¡°**:
```
data/cache/
â”œâ”€â”€ bybit_btcusdt_15m.parquet   âœ… ë‹¨ì¼ ì†ŒìŠ¤ (35,000+ ìº”ë“¤)
â”œâ”€â”€ bybit_ethusdt_15m.parquet   âœ…
â”œâ”€â”€ binance_btcusdt_15m.parquet âœ…
â””â”€â”€ bybit_btcusdt_1h.parquet    âš ï¸ DEPRECATED (ë ˆê±°ì‹œ)
```

---

### 2-2. Lazy Load ì €ì¥ ë°©ì‹ (Phase 1-C)

#### append_candle() - WebSocket ìº”ë“¤ ì¶”ê°€
**ìœ„ì¹˜**: `core/data_manager.py:377-411`

```python
def append_candle(self, candle: dict, save: bool = True):
    """ìƒˆ ìº”ë“¤ ì¶”ê°€ (Lazy Load ë°©ì‹)"""
    with self._data_lock:
        if self.df_entry_full is None:
            self.df_entry_full = pd.DataFrame()

        # 1. DataFrameìœ¼ë¡œ ë³€í™˜
        new_row = pd.DataFrame([candle])

        # Timestamp ì •ê·œí™”
        if 'timestamp' in new_row.columns:
            new_row['timestamp'] = pd.to_datetime(new_row['timestamp'])

        # 2. ë©”ëª¨ë¦¬ì— ì¶”ê°€ + ì¤‘ë³µ ì œê±°
        self.df_entry_full = pd.concat([self.df_entry_full, new_row], ignore_index=True)
        self.df_entry_full = self.df_entry_full.drop_duplicates(subset='timestamp', keep='last')
        self.df_entry_full = self.df_entry_full.sort_values('timestamp').reset_index(drop=True)

        # âœ… 3. Parquet ì €ì¥ ë¨¼ì € ìˆ˜í–‰ (ì „ì²´ íˆìŠ¤í† ë¦¬ ë³´ì¡´)
        if save:
            self._save_with_lazy_merge()

        # âœ… 4. ë©”ëª¨ë¦¬ truncateëŠ” ë‚˜ì¤‘ì— (ë©”ëª¨ë¦¬ ì ˆì•½)
        # Note: Parquetì€ ì´ë¯¸ ì „ì²´ ë°ì´í„°ë¥¼ ë³´ì¡´í–ˆìœ¼ë¯€ë¡œ ë©”ëª¨ë¦¬ë§Œ ì œí•œ
        if len(self.df_entry_full) > self.MAX_ENTRY_MEMORY:  # 1000ê°œ
            self.df_entry_full = self.df_entry_full.tail(self.MAX_ENTRY_MEMORY).reset_index(drop=True)
```

#### _save_with_lazy_merge() - Lazy Load ë³‘í•©
**ìœ„ì¹˜**: `core/data_manager.py:306-373`

```python
def _save_with_lazy_merge(self):
    """
    Lazy Load ë°©ì‹ìœ¼ë¡œ Parquet ì €ì¥

    - ë©”ëª¨ë¦¬: ìµœê·¼ 1000ê°œë§Œ ìœ ì§€ (40KB)
    - ì €ì¥ì†Œ: ì „ì²´ íˆìŠ¤í† ë¦¬ ë³´ì¡´ (35,000+ê°œ, 280KB)
    - ì„±ëŠ¥: 35ms I/O (15ë¶„ë‹¹ 1íšŒ)
    """
    try:
        entry_file = self.get_entry_file_path()

        # 1. ê¸°ì¡´ Parquet ì½ê¸°
        if entry_file.exists():
            df_old = pd.read_parquet(entry_file)

            # Timestamp ì •ê·œí™”
            if 'timestamp' in df_old.columns:
                if pd.api.types.is_numeric_dtype(df_old['timestamp']):
                    df_old['timestamp'] = pd.to_datetime(df_old['timestamp'], unit='ms', utc=True)
                else:
                    df_old['timestamp'] = pd.to_datetime(df_old['timestamp'])
        else:
            df_old = pd.DataFrame()

        # 2. ë³‘í•© + ì¤‘ë³µ ì œê±°
        df_merged = pd.concat([df_old, self.df_entry_full], ignore_index=True)
        df_merged = df_merged.drop_duplicates(subset='timestamp', keep='last')
        df_merged = df_merged.sort_values('timestamp').reset_index(drop=True)

        # 3. Parquet ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ int64 ë³€í™˜)
        save_df = df_merged.copy()
        if 'timestamp' in save_df.columns:
            if pd.api.types.is_datetime64_any_dtype(save_df['timestamp']):
                save_df['timestamp'] = save_df['timestamp'].astype(np.int64) // 10**6

        save_df.to_parquet(entry_file, index=False, compression='zstd')
        logging.debug(f"[DATA] Saved 15m: {entry_file.name} ({len(save_df)} candles)")

        # Bithumb -> Upbit ë³µì œ (í•œêµ­ ê±°ë˜ì†Œ í˜¸í™˜)
        if self.exchange_name == 'bithumb':
            try:
                upbit_file = self.cache_dir / f"upbit_{self.symbol_clean}_15m.parquet"
                import shutil
                shutil.copy(entry_file, upbit_file)
                logging.debug(f"[DATA] Replicated to Upbit: {upbit_file.name}")
            except Exception as e:
                logging.warning(f"[DATA] Upbit replication failed: {e}")

    except Exception as e:
        logging.error(f"[DATA] Lazy merge save failed: {e}", exc_info=True)
```

**Lazy Load íë¦„ë„**:
```
WebSocket ìº”ë“¤ ë„ì°©
  â†“
append_candle()
  â”œâ”€ 1. ë©”ëª¨ë¦¬ ì¶”ê°€ (df_entry_full)
  â”œâ”€ 2. _save_with_lazy_merge()
  â”‚     â”œâ”€ Parquet ì½ê¸° (35,000ê°œ)
  â”‚     â”œâ”€ ë³‘í•© + ì¤‘ë³µ ì œê±°
  â”‚     â””â”€ Parquet ì €ì¥ (35,001ê°œ)
  â””â”€ 3. ë©”ëª¨ë¦¬ truncate (1000ê°œ ìœ ì§€)
```

**ì„±ëŠ¥ íŠ¹ì„±**:
| í•­ëª© | ìˆ˜ì¹˜ |
|------|------|
| ë©”ëª¨ë¦¬ ì‚¬ìš© | 40KB (1000ê°œ) |
| Parquet í¬ê¸° | 280KB (35,000ê°œ) |
| ì½ê¸° ì‹œê°„ | 5-15ms |
| ì €ì¥ ì‹œê°„ | 25-50ms |
| CPU ë¶€í•˜ | 0.0039% (15ë¶„ë‹¹ 1íšŒ) |

---

### 2-3. ë‹¨ìˆœ ì €ì¥ ë°©ì‹ (save_parquet)

**ìœ„ì¹˜**: `core/data_manager.py:256-303`

```python
def save_parquet(self):
    """í˜„ì¬ ë°ì´í„°ë¥¼ Parquetìœ¼ë¡œ ì €ì¥ (FULL HISTORY)"""
    try:
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # 15m ë°ì´í„° ì €ì¥ (FULL HISTORY - NO TRUNCATION)
        if self.df_entry_full is not None and len(self.df_entry_full) > 0:
            entry_file = self.get_entry_file_path()
            save_df = self.df_entry_full.copy()  # FULL HISTORY

            # Timestamp ì²˜ë¦¬ (ms ì •ìˆ˜ë¡œ)
            if 'timestamp' in save_df.columns:
                if pd.api.types.is_datetime64_any_dtype(save_df['timestamp']):
                    save_df['timestamp'] = save_df['timestamp'].astype(np.int64) // 10**6

            save_df.to_parquet(entry_file, index=False, compression='zstd')
            logging.debug(f"[DATA] Saved 15m: {entry_file.name} ({len(save_df)} candles)")

        # 1h ë°ì´í„° ì €ì¥ (DEPRECATED)
        if self.df_pattern_full is not None and len(self.df_pattern_full) > 0:
            pattern_file = self.get_pattern_file_path()
            p_save_df = self.df_pattern_full.copy()

            if 'timestamp' in p_save_df.columns:
                if pd.api.types.is_datetime64_any_dtype(p_save_df['timestamp']):
                    p_save_df['timestamp'] = p_save_df['timestamp'].astype(np.int64) // 10**6

            p_save_df.to_parquet(pattern_file, index=False, compression='zstd')
            logging.debug(f"[DATA] Saved 1h: {pattern_file.name} ({len(p_save_df)} candles)")

    except Exception as e:
        logging.error(f"[DATA] Save error: {e}")
```

**ì‚¬ìš© ì‹œì **:
- ì´ˆê¸° REST API ë°ì´í„° ì €ì¥
- Backfill í›„ ì €ì¥
- ìˆ˜ë™ ì €ì¥ ìš”ì²­

---

## 3. ë°ì´í„° ì½ê¸°

### 3-1. ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ì½ê¸°

#### ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘
**ìœ„ì¹˜**: `core/multi_backtest.py` (ì¶”ì •)

```python
def run_backtest(exchange_name: str, symbol: str, params: dict):
    # 1. ë°ì´í„° ë§¤ë‹ˆì € ìƒì„±
    data_manager = BotDataManager(exchange_name, symbol)

    # 2. Parquet ë¡œë“œ
    success = data_manager.load_historical(
        fetch_callback=lambda: exchange.get_klines('15m', 1000)
    )

    if not success:
        logging.error("Failed to load data")
        return None

    # 3. ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_15m = data_manager.df_entry_full      # 15ë¶„ë´‰ (ë©”ëª¨ë¦¬)
    df_1h = data_manager.df_pattern_full     # 1ì‹œê°„ë´‰ (ë¦¬ìƒ˜í”Œë§)

    # 4. ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = strategy.run_backtest(df_15m, df_1h, params)
    return results
```

#### ë°ì´í„° ì ‘ê·¼
```python
# âœ… ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸° (ë¹ ë¦„)
df_15m = data_manager.df_entry_full  # ìµœê·¼ 1000ê°œ ë˜ëŠ” ì „ì²´ (ì´ˆê¸° ë¡œë“œ ì‹œ)

# âœ… ë¦¬ìƒ˜í”Œë§ (15m â†’ 1h)
df_1h = data_manager.resample_data(df_15m, '1h')

# âŒ ë ˆê±°ì‹œ (ì‚¬ìš© ì§€ì–‘)
df_1h = data_manager.df_pattern_full  # DEPRECATED
```

---

### 3-2. ì‹¤ì‹œê°„ ë§¤ë§¤ ë°ì´í„° ì½ê¸°

#### ì‹ í˜¸ ê°ì§€ ì‹œ ë°ì´í„° ì‚¬ìš©
**ìœ„ì¹˜**: `core/unified_bot.py:333-343`

```python
def detect_signal(self) -> Optional[Signal]:
    if not hasattr(self, 'mod_signal'):
        return None

    # 1. í˜„ì¬ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸°
    candle = self.exchange.get_current_candle()

    # 2. ë©”ëª¨ë¦¬ì—ì„œ ë°ì´í„° ì½ê¸°
    df_pattern = self.df_pattern_full if self.df_pattern_full is not None else pd.DataFrame()
    df_entry = self.df_entry_resampled if self.df_entry_resampled is not None else pd.DataFrame()

    # 3. ë§¤ë§¤ ì¡°ê±´ í™•ì¸
    cond = self.mod_signal.get_trading_conditions(df_pattern, df_entry)

    # 4. ì§„ì… ì²´í¬
    action = self.mod_position.check_entry_live(self.bt_state, candle, cond, self.df_entry_resampled)

    if action and action.get('action') == 'ENTRY':
        return Signal(
            type=action['direction'],
            pattern=action['pattern'],
            stop_loss=action.get('sl', 0),
            atr=action.get('atr', 0.0)
        )
    return None
```

**ë°ì´í„° íë¦„**:
```
ë©”ëª¨ë¦¬ (df_entry_full, df_pattern_full)
  â†“
ì‹ í˜¸ í”„ë¡œì„¸ì„œ (mod_signal)
  â†“
ë§¤ë§¤ ì¡°ê±´ í™•ì¸ (3-Filter)
  â†“
ì§„ì… ì‹ í˜¸ ë°˜í™˜
```

---

### 3-3. í¬ì§€ì…˜ ê´€ë¦¬ ì‹œ ë°ì´í„° ì‚¬ìš©

**ìœ„ì¹˜**: `core/unified_bot.py:354-363`

```python
def manage_position(self):
    if not self.position:
        return

    # 1. í˜„ì¬ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸°
    candle = self.exchange.get_current_candle()

    # 2. í¬ì§€ì…˜ ê´€ë¦¬ (ì†ì ˆ/ìµì ˆ ì²´í¬)
    res = self.mod_position.manage_live(
        self.bt_state,
        candle,
        self.df_entry_resampled  # â† ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
    )

    # 3. ì²­ì‚° ì‹¤í–‰
    if res and res.get('action') == 'CLOSE':
        exit_price = res.get('price', candle.get('close', 0.0))
        if self.mod_order.execute_close(self.position, exit_price, reason=res.get('reason', 'UNKNOWN'), bt_state=self.bt_state):
            self.position = None
            if self.exchange:
                self.exchange.position = None
            self.save_state()
```

**ë°ì´í„° íë¦„**:
```
ë©”ëª¨ë¦¬ (df_entry_resampled)
  â†“
í¬ì§€ì…˜ ë§¤ë‹ˆì € (mod_position)
  â†“
ì†ì ˆ/ìµì ˆ ì²´í¬
  â†“
ì²­ì‚° ì‹ í˜¸ ë°˜í™˜
```

---

## 4. ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥

### 4-1. ìŠ¤ë ˆë“œ ì•ˆì „

**ìœ„ì¹˜**: `core/data_manager.py:88`

```python
# ìŠ¤ë ˆë“œ ì•ˆì „
self._data_lock = threading.RLock()
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
def append_candle(self, candle: dict, save: bool = True):
    with self._data_lock:  # â† ë½ íšë“
        # ë°ì´í„° ì¶”ê°€/ì €ì¥ ì‘ì—…
        self.df_entry_full = pd.concat([...])
        self._save_with_lazy_merge()
```

---

### 4-2. ì¤‘ë³µ ì œê±°

**ìœ„ì¹˜**: ëª¨ë“  ì €ì¥ ë¡œì§

```python
# ì¤‘ë³µ ì œê±° (timestamp ê¸°ì¤€)
df_merged = df_merged.drop_duplicates(subset='timestamp', keep='last')
df_merged = df_merged.sort_values('timestamp').reset_index(drop=True)
```

**ì‹œë‚˜ë¦¬ì˜¤**:
```
ê¸°ì¡´ Parquet:
  2024-01-16 08:00:00  43000.0  ...
  2024-01-16 08:15:00  43200.0  ...

WebSocket ì¶”ê°€:
  2024-01-16 08:15:00  43250.0  ... (ìˆ˜ì •ëœ ê°’)
  2024-01-16 08:30:00  43500.0  ...

ë³‘í•© í›„:
  2024-01-16 08:00:00  43000.0  ...
  2024-01-16 08:15:00  43250.0  ... â† keep='last'ë¡œ ìµœì‹ ê°’ ìœ ì§€
  2024-01-16 08:30:00  43500.0  ...
```

---

### 4-3. Timestamp ì •ê·œí™”

**ìœ„ì¹˜**: ëª¨ë“  ì½ê¸°/ì €ì¥ ë¡œì§

```python
# ì½ê¸° ì‹œ: int64 ms â†’ datetime
if pd.api.types.is_numeric_dtype(df['timestamp']):
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)

# ì €ì¥ ì‹œ: datetime â†’ int64 ms
if pd.api.types.is_datetime64_any_dtype(save_df['timestamp']):
    save_df['timestamp'] = save_df['timestamp'].astype(np.int64) // 10**6
```

**ë³€í™˜ ê³¼ì •**:
```
API ì‘ë‹µ:      "1705401600000" (string)
  â†“ int ë³€í™˜
ë©”ëª¨ë¦¬:        1705401600000 (int64)
  â†“ datetime ë³€í™˜
ì²˜ë¦¬:          Timestamp('2024-01-16 08:00:00', tz='UTC')
  â†“ int64 ë³€í™˜
Parquet ì €ì¥:  1705401600000 (int64)
```

---

## 5. ì„±ëŠ¥ íŠ¹ì„±

### 5-1. ë©”ëª¨ë¦¬ ì‚¬ìš©

| ë°ì´í„° | í¬ê¸° | ê°œìˆ˜ |
|--------|------|------|
| df_entry_full | 40KB | 1,000ê°œ (15ë¶„ë´‰) |
| df_entry_resampled | 10KB | 250ê°œ (1ì‹œê°„ë´‰) |
| df_pattern_full | 15KB | 500ê°œ (1ì‹œê°„ë´‰) |
| **ì´í•©** | **65KB** | **1,750ê°œ** |

---

### 5-2. ë””ìŠ¤í¬ ì‚¬ìš©

| íŒŒì¼ | í¬ê¸° | ê°œìˆ˜ | ì••ì¶•ë¥  |
|------|------|------|--------|
| 15m Parquet | 280KB | 35,000ê°œ | 92% (zstd) |
| ë©”ëª¨ë¦¬ (ë¹„ì••ì¶•) | 3,500KB | 35,000ê°œ | - |

---

### 5-3. I/O ì„±ëŠ¥

| ì‘ì—… | ì‹œê°„ | ë¹ˆë„ | ì˜í–¥ |
|------|------|------|------|
| Parquet ì½ê¸° | 5-15ms | ì‹œì‘ ì‹œ 1íšŒ | ë¬´ì‹œ ê°€ëŠ¥ |
| Parquet ì €ì¥ | 25-50ms | 15ë¶„ë‹¹ 1íšŒ | ë¬´ì‹œ ê°€ëŠ¥ |
| Backfill | 100-300ms | 5ë¶„ë‹¹ 1íšŒ | ë¬´ì‹œ ê°€ëŠ¥ |
| WebSocket ì¶”ê°€ | 35ms | 15ë¶„ë‹¹ 1íšŒ | ë¬´ì‹œ ê°€ëŠ¥ |

**CPU ë¶€í•˜**: 0.0039% (15ë¶„ë‹¹ 35ms Ã· 900ì´ˆ)

---

## 6. ì—ëŸ¬ ì²˜ë¦¬

### 6-1. API ì—ëŸ¬

```python
# âŒ í˜„ì¬: ì—ëŸ¬ ì‹œ None ë°˜í™˜
def get_klines(...) -> Optional[pd.DataFrame]:
    try:
        result = self.session.get_kline(...)
        if result.get('retCode') != 0:
            logging.error(f"Kline error: {result.get('retMsg')}")
            return None  # â† ì—ëŸ¬ ìˆ¨ê¹€
    except Exception as e:
        logging.error(f"Kline fetch error: {e}")
        return None

# âœ… ê¶Œì¥: ì˜ˆì™¸ ë°œìƒ
def get_klines(...) -> pd.DataFrame:
    try:
        result = self.session.get_kline(...)
        if result.get('retCode') != 0:
            raise RuntimeError(f"Kline API error: {result.get('retMsg')}")
        return df
    except Exception as e:
        raise RuntimeError(f"Cannot fetch klines: {e}") from e
```

---

### 6-2. ì €ì¥ ì‹¤íŒ¨

```python
# âœ… í˜„ì¬: try-exceptë¡œ ë³´í˜¸
def save_parquet(self):
    try:
        save_df.to_parquet(entry_file, index=False, compression='zstd')
        logging.debug(f"[DATA] Saved {len(save_df)} candles")
    except Exception as e:
        logging.error(f"[DATA] Save error: {e}")
        # ë©”ëª¨ë¦¬ ë°ì´í„°ëŠ” ìœ ì§€ë¨ (ë°ì´í„° ì†ì‹¤ ì—†ìŒ)
```

**ì•ˆì „ì„±**:
- ì €ì¥ ì‹¤íŒ¨ ì‹œ: ë©”ëª¨ë¦¬ ë°ì´í„° ìœ ì§€ â†’ ë‹¤ìŒ ì €ì¥ ì‹œë„ì—ì„œ ë³µêµ¬
- ë©”ëª¨ë¦¬ ì†ì‹¤ ì—†ìŒ

---

### 6-3. ë¡œë“œ ì‹¤íŒ¨

```python
# âœ… í˜„ì¬: REST API í´ë°±
def load_historical(self, fetch_callback):
    if entry_file.exists():
        df = pd.read_parquet(entry_file)
        # ...
    else:
        logging.warning(f"[DATA] Parquet not found")
        if fetch_callback:
            df_rest = fetch_callback()  # REST API í´ë°±
            # ...
```

**ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤**:
```
Parquet íŒŒì¼ ì—†ìŒ
  â†“
REST API í˜¸ì¶œ (1000ê°œ)
  â†“
Parquet ì €ì¥
  â†“
ì •ìƒ ë™ì‘
```

---

## 7. ë°ì´í„° íë¦„ ì¢…í•©

### 7-1. ì´ˆê¸° ì‹œì‘

```
í”„ë¡œê·¸ë¨ ì‹œì‘
  â†“
BotDataManager ìƒì„±
  â†“
load_historical()
  â”œâ”€ Parquet ì¡´ì¬? â†’ ì½ê¸° (5-15ms)
  â””â”€ ì—†ìŒ? â†’ REST API (1-3ì´ˆ) â†’ Parquet ì €ì¥
  â†“
WebSocket ì‹œì‘
  â”œâ”€ on_connect: Backfill ì‹¤í–‰
  â””â”€ on_candle_close: append_candle()
  â†“
Data Monitor ì‹œì‘ (5ë¶„ ì£¼ê¸°)
  â””â”€ Backfill ì‹¤í–‰
  â†“
ë©”ì¸ ë£¨í”„
  â”œâ”€ ì‹ í˜¸ ê°ì§€ (ë©”ëª¨ë¦¬ ì½ê¸°)
  â””â”€ í¬ì§€ì…˜ ê´€ë¦¬ (ë©”ëª¨ë¦¬ ì½ê¸°)
```

---

### 7-2. ì‹¤ì‹œê°„ ìš´ì˜

```
WebSocket 15ë¶„ë´‰ ì™„ë£Œ
  â†“
_on_candle_close()
  â†“
append_candle()
  â”œâ”€ 1. ë©”ëª¨ë¦¬ ì¶”ê°€
  â”œâ”€ 2. _save_with_lazy_merge()
  â”‚     â”œâ”€ Parquet ì½ê¸° (5-15ms)
  â”‚     â”œâ”€ ë³‘í•© (1-2ms)
  â”‚     â””â”€ Parquet ì €ì¥ (25-50ms)
  â””â”€ 3. ë©”ëª¨ë¦¬ truncate (1000ê°œ)
  â†“
_process_historical_data()
  â”œâ”€ ì§€í‘œ ì¬ê³„ì‚°
  â””â”€ íŒ¨í„´ ì‹œê·¸ë„ ì¶”ê°€
  â†“
ë©”ì¸ ë£¨í”„ ê³„ì†
```

---

### 7-3. Backfill (5ë¶„ ì£¼ê¸°)

```
Data Monitor (5ë¶„ ëŒ€ê¸°)
  â†“
ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ í™•ì¸
  â†“
í˜„ì¬ ì‹œê°„ - ë§ˆì§€ë§‰ ì‹œê°„ > 15ë¶„?
  â†“ YES
REST API í˜¸ì¶œ (ëˆ„ë½ ê°œìˆ˜ + 10)
  â†“
timestamp í•„í„°ë§ (ë§ˆì§€ë§‰ ì´í›„ë§Œ)
  â†“
ë©”ëª¨ë¦¬ ì¶”ê°€ + Parquet ì €ì¥
  â†“
ë©”ëª¨ë¦¬ truncate (1000ê°œ)
```

---

## 8. ê²€ì¦ ê²°ê³¼

### âœ… ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ëœ ë¶€ë¶„

| í•­ëª© | ìƒíƒœ |
|------|------|
| **API ë°ì´í„° ìˆ˜ì§‘** | âœ… REST + WebSocket ì´ì¤‘í™” |
| **ë°ì´í„° ì €ì¥** | âœ… Lazy Load (ë©”ëª¨ë¦¬/ì €ì¥ì†Œ ë¶„ë¦¬) |
| **ë°ì´í„° ì½ê¸°** | âœ… ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì½ê¸° |
| **ë¬´ê²°ì„±** | âœ… ì¤‘ë³µ ì œê±° + Timestamp ì •ê·œí™” |
| **ìŠ¤ë ˆë“œ ì•ˆì „** | âœ… RLock ì‚¬ìš© |
| **ì„±ëŠ¥** | âœ… 35ms I/O (15ë¶„ë‹¹ 1íšŒ) |
| **ë³µêµ¬** | âœ… Backfill (5ë¶„ ì£¼ê¸°) + REST í´ë°± |

---

### âš ï¸ ë°œê²¬ëœ ë¬¸ì œ

**API ì—ëŸ¬ ì²˜ë¦¬ ëˆ„ë½** (30+ ì§€ì )
- `get_klines()` ì‹¤íŒ¨ ì‹œ `None` ë°˜í™˜
- `get_current_price()` ì‹¤íŒ¨ ì‹œ `0.0` ë°˜í™˜
- í˜¸ì¶œ ì½”ë“œì—ì„œ ì²´í¬ ì—†ì´ ì‚¬ìš©

**ê¶Œì¥ í•´ê²°**:
```python
# âœ… ì˜ˆì™¸ ë°œìƒ ë°©ì‹
def get_klines(...) -> pd.DataFrame:
    try:
        ...
    except Exception as e:
        raise RuntimeError(f"Cannot fetch klines: {e}") from e
```

---

## 9. ê²°ë¡ 

> **ì§ˆë¬¸**: "API - ë°ì´í„° ì €ì¥, ì½ê¸°, ì´ ë‚´ìš© ê¸°ì¤€ìœ¼ë¡œë§Œ ë¶„ì„"
>
> **ë‹µë³€**: âœ… **ì „ì²´ íë¦„ì´ ê²¬ê³ í•˜ê²Œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤!**

**ê°•ì **:
- âœ… REST + WebSocket ì´ì¤‘í™” (ì•ˆì •ì„±)
- âœ… Lazy Load ì•„í‚¤í…ì²˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
- âœ… Parquet ì••ì¶• ì €ì¥ (92% ì••ì¶•ë¥ )
- âœ… Backfill ìë™ ë³µêµ¬ (5ë¶„ ì£¼ê¸°)
- âœ… ìŠ¤ë ˆë“œ ì•ˆì „ (RLock)
- âœ… ì¤‘ë³µ ì œê±° + ì •ê·œí™” (ë¬´ê²°ì„±)

**ì•½ì **:
- âš ï¸ API ì—ëŸ¬ ì²˜ë¦¬ ëˆ„ë½ (30+ ì§€ì )
- âš ï¸ ë ˆê±°ì‹œ 1h Parquet (SSOT ìœ„ë°°)

**ì‹¤ê±°ë˜ ì¤€ë¹„ë„**: 85%
- âœ… ë°ì´í„° ìˆ˜ì§‘: ì™„ë²½
- âœ… ë°ì´í„° ì €ì¥: ì™„ë²½
- âœ… ë°ì´í„° ì½ê¸°: ì™„ë²½
- âš ï¸ ì—ëŸ¬ ì²˜ë¦¬: ë³´ì™„ í•„ìš”

---

**ì‘ì„±**: Claude Sonnet 4.5 (2026-01-15)
**ê²€ì¦**: ì‹¤ì œ ì½”ë“œ ë¶„ì„ (exchanges/, core/data_manager.py, core/unified_bot.py)
